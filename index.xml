<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Learning, Thinking, and Coding</title>
<link>https://brianrepko.github.io/blog/</link>
<atom:link href="https://brianrepko.github.io/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Brian Repko&#39;s blog</description>
<generator>quarto-1.8.27</generator>
<lastBuildDate>Thu, 15 Jan 2026 00:00:00 GMT</lastBuildDate>
<item>
  <title>Dataset-JSON</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2026-01-15-datasetjson-part2/</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Series
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is part 2 of a series on dataset-json, you can find part 1 <a href="https://brianrepko.github.io/blog/posts/2025-12-31-datasetjson-part1/" target="_blank">here</a> for an overview of dataset-json.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>CDISC workshop
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is a planned Dataset-JSON Hands-On Implementation workshop on 18 May 2026 from 0900-1300 CET at the 2026 CDISC Europe Interchange conference in Milan. See <a href="https://www.cdisc.org/events/interchange/2026-cdisc-europe-interchange">the main website</a> or <a href="https://web.cvent.com/event/b32aaba1-214b-486e-8b5a-fdc8342f9794/websitePage:645d57e4-75eb-4769-b2c0-f201a0bfc6ce">here</a> for details.</p>
</div>
</div>
<section id="intention" class="level2">
<h2 class="anchored" data-anchor-id="intention">Intention</h2>
<p>Similar to how art can be additive (painting) or subtractive (sculpting), computational work can be positive (‚Äúhow do I get to a result‚Äù) or negative (‚Äúwhat could go wrong here‚Äù). Most of us develop in both ways of working - and we tend to have a default / stronger side. Mine is the negative - which is why I think I‚Äôve always focused on risk with systems development. I mention this because this blog post falls 100% in the - what could go wrong here category. This is me going through a specification and calling out anything that feels risky. I hope this spawns conversations and github issues and wiki updates and PRs. This is me also calling out where I‚Äôd love to contribute - not just complain. I‚Äôll also add that I am not a SAS programmer so I may definitely get some items wrong here.</p>
<p>From part 1, here is the <a href="https://cdisc-org.github.io/DataExchange-DatasetJson/doc/dataset-json1-1.html">link to the html version of the spec</a> - so let‚Äôs dive in‚Ä¶</p>
</section>
<section id="background---xml-and-json-technologies" class="level2">
<h2 class="anchored" data-anchor-id="background---xml-and-json-technologies">Background - XML and JSON technologies</h2>
<p>Within the XML set of specifications and technologies - we have the following:</p>
<ul>
<li><a href="https://www.w3.org/TR/xml/">XML</a> - data format</li>
<li><a href="https://www.w3.org/TR/xmlschema11-2/">XML Schema</a> - structure and data types with simple validation and constraints</li>
<li><a href="https://schematron.com">Schematron</a> - rule-based content validation, complex relationships, and business logic</li>
</ul>
<p>For JSON we have a similar set of specifications and technologies:</p>
<ul>
<li><a href="https://www.json.org/json-en.html">JSON</a> - data format</li>
<li><a href="https://json-schema.org">JSON schema</a> - structure and data types with simple validation and constraints</li>
<li><a href="https://amer-ali.github.io/jsontron/">jsontron</a> - a port of Schematron for JSON (not widely used)</li>
</ul>
<p>The reason that I want to bring this up is that some of the issues below relate to ‚Äúwhere‚Äù something belongs and potentially having it in the wrong space.</p>
<p>Also, I think that the use of jsontron would be interesting in this space potentially.</p>
<section id="data-types" class="level3">
<h3 class="anchored" data-anchor-id="data-types">Data Types</h3>
<p>JSON, JSON schema, and dataset-json work with a set of data types. Note that JSON / JSON schema are working on object / property definition and dataset-json is column definition. Here is a table that goes over what is available for each specification in relation to each other:</p>
<table class="caption-top table">
<caption>Data Types</caption>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>JSON</th>
<th>JSON schema</th>
<th>dataset-json</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>boolean</td>
<td>boolean</td>
<td>boolean</td>
</tr>
<tr class="even">
<td>string</td>
<td>string</td>
<td>string</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>decimal</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>datetime</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>date</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>time</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>URI</td>
</tr>
<tr class="even">
<td>number</td>
<td>integer</td>
<td>integer</td>
</tr>
<tr class="odd">
<td></td>
<td>number</td>
<td>float</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>double</td>
</tr>
<tr class="odd">
<td>array</td>
<td>array</td>
<td>-</td>
</tr>
<tr class="even">
<td>object</td>
<td>object</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Note that dataset-json has both a JSON schema and LinkML version of the schema. The JSON schema is based on the 2019-09 standard (vs the latest, which is 2020-12).</p>
<p>Also note that JSON schema has various constraints added to the type. In general, objects can also specify, via a <code>required</code> array, which properties are required. There is also the capability to do ‚Äúrequired-if‚Äù logic (which could also be a jsontron rule).</p>
<p>For string-based data types, we have <code>minLength</code>, <code>maxLength</code>, <code>pattern</code>, and <code>format</code>. JSON schema has known formats for <code>date-time</code>, <code>date</code>, <code>time</code>, and <code>uri</code> (and <code>iri</code>) - but <strong>dataset-json does not use these</strong>. It makes use of <code>pattern</code> for validation of those string-based types. This may be the case that <code>pattern</code> is actually used when validating and <code>format</code> may not be. That is probably dependent on the validator software. You can find the full list of supported JSON schema formats <a href="https://json-schema.org/understanding-json-schema/reference/type#built-in-formats">here</a>.</p>
<p>For numeric data, JSON schema separates integers (‚Ñ§) from decimal (ùîª) data. Note that decimals (ùîª) is a subset of the rationals (‚Ñö) - given by <img src="https://latex.codecogs.com/png.latex?a/10%5En"> (where <img src="https://latex.codecogs.com/png.latex?a"> and <img src="https://latex.codecogs.com/png.latex?n"> are in ‚Ñ§). Thus <img src="https://latex.codecogs.com/png.latex?1/3"> is in ‚Ñö but not in ùîª (it repeats forever) and thus for some values we have to deal with both precision and rounding.</p>
<p>The dataset-json data types of <code>float</code> and <code>double</code> are intended to represent in-memory IEEE 754 single- and double-precision constructs. It is basically saying what data type to use in the environment (SAS, R, python, etc.) that will import this data - or at least the <em>capability</em> of said environment (eg. R only has doubles - not floats). There are the well-known issues of converting values in ùîª to a base 2 binary format.</p>
<p>Interestingly, there is no JSON schema format for dataset-json‚Äôs ‚Äúdecimal‚Äù - again that is using a data type to specify the in-memory representation for this column (aka decimal stored as a string).</p>
<p>In terms of character encoding for the file, later versions of JSON require UTF-8 encoding. The dataset-json User‚Äôs Guide refers to optional usage of UTF-16 or UTF-32 but this is not allowed in <a href="https://datatracker.ietf.org/doc/html/rfc8259">RFC8259</a>.</p>
<p>Also, a general issue with JSON is that the Unicode escape - <code>\uXXXX</code> - only supports Unicode‚Äôs Basic Multilingual Plane (BMP) and not full Unicode. One needs to either embed the codepoints directly (UTF-8 supports all Unicode) or escape as surrogate pair.</p>
<p>And then for moving from US-ASCII to Unicode‚Ä¶</p>
<p><img src="https://brianrepko.github.io/blog/posts/2026-01-15-datasetjson-part2/unicode-meme.jpg" class="img-fluid"></p>
<p>Once you have Unicode strings, there are many issues that one needs to worry about. I‚Äôm not going to go over all of these but here is a short list (many from <a href="https://tonsky.me/blog/unicode/">here</a>).</p>
<ul>
<li>Unicode normalization</li>
<li>length of a Unicode string (with grapheme clusters - particularly for Asian languages)</li>
<li>casing (upper/lower)</li>
<li>sorting / collation</li>
<li>right-to-left vs left-to-right languages</li>
<li>regex class membership (eg. <code>:alpha:</code>)</li>
</ul>
<p>I get that we are looking to move beyond the current XPT limitations at a later stage. At some point, I‚Äôm sure CDISC will have a number of workshops on ‚ÄúInternationalizing Clinical Trial Data‚Äù.</p>
</section>
</section>
<section id="our-problem-domain" class="level2">
<h2 class="anchored" data-anchor-id="our-problem-domain">Our Problem Domain</h2>
<p>I think it‚Äôs useful to also step back and share what the actual problem we are trying to solve here is. We will have, as part of the data and coding related to a clinical trial, ‚Äúdata frames‚Äù. For SAS, these are datasets, for R, these are data.frame-s and for python, these are pandas.DataFrame-s (or polars?) and in SQL, these are tables.</p>
<p>We have two main contexts - same-language serialization (export / import) and different-language serialization. In both these contexts, the ultimate question is - is the original in-memory data frame the same as the resulting in-memory data frame. When we are in the same-language context (SAS ‚Üí JSON ‚Üí SAS), ‚Äúsameness‚Äù of in-memory data frames can consider deeper properties of the internal data structure. When we are in the different-language context (R ‚Üí JSON ‚Üí SAS), ‚Äúsameness‚Äù of the in-memory data frames does not have access to those deeper properties - internal data types used for the columns are in different programming languages and data frame classes have different properties - SAS datasets can have a ‚Äúkey‚Äù (hash), multiple indices, display formats etc. R data.frames have row.names which SAS does not (similar to key but SAS keys don‚Äôt <em>have to</em> be unique).</p>
<p>The reason that I bring this up is that some parts of the dataset-json specification are for the <strong>SAS-specific same-language context</strong>.</p>
<p>I actually think that we might not care about a different-language context as clinical trial validation from the regulatory authority will be same-language. However, as clinical trial code becomes more polyglot - some SAS, some R, some python - then we might need to consider different-language contexts.</p>
</section>
<section id="oids-and-the-model-beyond-a-data-frame" class="level2">
<h2 class="anchored" data-anchor-id="oids-and-the-model-beyond-a-data-frame">OIDs and the model beyond a ‚Äúdata frame‚Äù</h2>
<p>As I was initially reviewing the dataset-json specification, one quick item that stuck out was that there was no way to say if a column allowed for missing values or not - that is - is a column required.</p>
<p>This led me into the ODM model and the various OID columns in the specification.</p>
<p>The dataset-json specification is not <em>ONLY</em> ‚Äúserialized data frames‚Äù - it is ‚Äúserialized data frames within a catalog‚Äù. That catalog is the <code>define.xml</code> that is sent along with the <code>.json</code> or <code>.ndjson</code> or <code>.dsjc</code> files. This gets a bit too deep and needs it‚Äôs own blog post but in short - you can think of the following connections.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>ODM term</th>
<th>concept</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Study</td>
<td>Study</td>
</tr>
<tr class="even">
<td>ItemGroup</td>
<td>Domain or data frame metadata</td>
</tr>
<tr class="odd">
<td>Item</td>
<td>Variable or column metadata</td>
</tr>
<tr class="even">
<td>ItemGroupData</td>
<td>actual row of values</td>
</tr>
<tr class="odd">
<td>ItemData</td>
<td>actual cell value</td>
</tr>
<tr class="even">
<td>File</td>
<td>actual filename</td>
</tr>
</tbody>
</table>
<p>So an ItemGroupOID might have a value of <code>IG.DM</code> for the SDTM <code>DM</code> domain (Demographics). And there will be a ItemOID of <code>IT.STUDYID</code> or <code>IT.USUBJID</code> in that item group. Those are columns that used in multiple domains. The DM domain might also have an ItemOID of <code>IT.DM.ETHNIC</code> which is specific to the DM domain.</p>
<p>The reason that I bring this up is that some properties of the column definition are in <code>define.xml</code> as ItemRef(s) and some properties as ItemDef(s). <code>Mandatory</code> is part of the ItemRef. The dataset-json specification is only ItemDef data.</p>
<p>The specification is written so that the OID columns are optional, but to fully validate data, there are some column attributes that are only in <code>define.xml</code>.</p>
</section>
<section id="column-attribute-issues" class="level2">
<h2 class="anchored" data-anchor-id="column-attribute-issues">Column Attribute Issues</h2>
<p>Looking at the column attributes, what is required are the basics</p>
<ul>
<li><code>itemOID</code> - the unique key of this column</li>
<li><code>name</code> - the column name</li>
<li><code>label</code> - the column label</li>
<li><code>dataType</code> - the column type</li>
</ul>
<p>These all make sense - with dataType being an enum (see list above). Technically, there are many other dataTypes in the ODM XML model but in practice the main types are there.</p>
<section id="targetdatatype" class="level3">
<h3 class="anchored" data-anchor-id="targetdatatype">targetDataType</h3>
<p>As for <code>targetDataType</code>, this came about as datetime, date, and time values in SAS are <strong>either</strong> strings or numeric values (with the 0-date being Jan 1, 19<strong>60</strong> and the 0-time being midnight). But the data in the JSON file is always a string, so this column attribute is telling the serialization software what the resulting in-memory data type should be. But R and python have various classes for datetime, date, and time values. Some of those do use a number representation for timestamps under the covers. But again, it seems we are using the specification to specify details of the internal data structure.</p>
<p>This column attribute is also used for the <code>decimal</code> data type in a superfluous manner. It has to be added for string-representations of decimal values.</p>
<p>If we really want to specify the language-specific data structure properties, then it might make more sense to have those under the top-level <code>sourceSystem</code> perhaps - or under language-specific top-level attributes.</p>
</section>
<section id="length" class="level3">
<h3 class="anchored" data-anchor-id="length">length</h3>
<p>Next we have <code>length</code>, which as mentioned above is problematic for Unicode strings based on normalization and grapheme clustering. The sense I get is that the value here is meant to be a <strong>capability</strong> (you have to handle strings at-least this long).</p>
</section>
<section id="displayformat" class="level3">
<h3 class="anchored" data-anchor-id="displayformat">displayFormat</h3>
<p>This is clearly a SAS only attribute to used to make sure original and resulting datasets are the ‚Äúsame‚Äù.</p>
</section>
<section id="keysequence" class="level3">
<h3 class="anchored" data-anchor-id="keysequence">keySequence</h3>
<p>The keySequence attribute can be used to create the SAS dataset key (hash) - so this is clearly for internal data structure. Technically in SAS, this can doesn‚Äôt have to be unique (eg. <code>MULTIDATA: 'Y'</code>). For R, <code>row.names</code> could be created from this but <code>row.names</code> must be unique.</p>
<p>I‚Äôm not familiar enough with SAS to know if a component of a key implies it is required.</p>
<p>I‚Äôd also add that SAS datasets can have multiple indices, so I‚Äôm not sure why, if we are specifying internal data structure we include the key and not the indices. My guess is that these are not part of ODM XML schema to which we are creating this specification from. It could be the ODM XML schema is a bit SAS-specific as well.</p>
</section>
<section id="what-is-not-here" class="level3">
<h3 class="anchored" data-anchor-id="what-is-not-here">What is not here</h3>
<p>As mentioned above, if I needed to specify more rules to validate a dataset, then I would need to know if the column values can be missing or not (required) or conditionally required.</p>
<p>In addition, all those JSON schema properties start to apply - range on numeric values? enumerations? code-lists? string regex?</p>
</section>
</section>
<section id="top-level-metadata-attribute-issues" class="level2">
<h2 class="anchored" data-anchor-id="top-level-metadata-attribute-issues">Top-level Metadata Attribute Issues</h2>
<p>Again, the required attributes for the dataset-json top-level metadata all make sense.</p>
<ul>
<li>name - the name of the dataset</li>
<li>label - the label for the dataset</li>
<li>columns - the column definitions (see above)</li>
<li>records - the number of rows</li>
<li>datasetJSONCreationDateTime - timestamp for dataset-json file creation</li>
<li>datasetJSONVersion - the version (1.1 with optional third component)</li>
<li>itemGroupOID - the OID for this dataset</li>
</ul>
<p>In general, it is a pet peeve of mine to allow datetime and time values without timezones but that is allowed here (as well as dealing with incomplete data) - all part of ISO 8601.</p>
<p>And for style, I‚Äôd have used <code>rowCount</code> for <code>records</code> but given the context it makes total sense.</p>
<section id="oids" class="level3">
<h3 class="anchored" data-anchor-id="oids">OIDs</h3>
<p>The <code>fileOID</code>, <code>studyOID</code>, <code>metaDataVersionOID</code> and <code>metaDataRef</code> attributes are all related to connecting this data to the <code>define.xml</code> catalog - technically optional but I‚Äôm not sure how to properly validate a dataset without it.</p>
<p>In the end, <code>itemGroupOID</code> and <code>itemOID</code> are the only required OID values.</p>
</section>
<section id="dblastmodifieddatetime" class="level3">
<h3 class="anchored" data-anchor-id="dblastmodifieddatetime">dbLastModifiedDateTime</h3>
<p>The <code>dbLastModifiedDateTime</code> attribute is the last modified timestamp for the source of this JSON data.</p>
<p>There is a jsontron type rule for this being earlier than the creation timestamp required attribute.</p>
<p>Again as a style thing, I would have used ‚Äúsource‚Äù vs ‚Äúdb‚Äù - and perhaps made this part of the <code>sourceSystem</code> object. And it should have a timezone.</p>
<p>The mention of ‚Äúdb‚Äù however, makes me wonder about creating a DuckDB extension to read and write dataset-json files (each file as a table and then include the catalog data as well).</p>
</section>
<section id="source-information" class="level3">
<h3 class="anchored" data-anchor-id="source-information">Source information</h3>
<p>The <code>originator</code>, <code>sourceSystem</code> (with <code>name</code> and <code>version</code>) are to be used to share the organization and ‚Äúsystem‚Äù that generated this dataset-json file. It‚Äôs not clear to me if <code>sourceSystem</code> should be a package name, programming language, internal SCE name? I think more clarity on how this could be used is warranted. This could also be a place there SAS, R, python-specific data could be recorded.</p>
</section>
</section>
<section id="extendability-issues" class="level2">
<h2 class="anchored" data-anchor-id="extendability-issues">Extendability Issues</h2>
<p>There is discussion of extendability to dataset-json - this uses a new <code>sourceSystem.systemExtensions</code> attribute but also seems to imply a custom JSON schema file. I do think JSON schema has built in schema extension capabilities and perhaps those could be looked at in the examples. Perhaps this is a place to extend the data to internal data structure details.</p>
</section>
<section id="next-article" class="level2">
<h2 class="anchored" data-anchor-id="next-article">Next article</h2>
<p>I hope this is useful and I am more than open to creating github issues / updates to the User Guide etc. Happy to receive feedback - my contact info is at <a href="https://www.learnthinkcode.com">my website</a>.</p>
<p>In part 3, I‚Äôll review the API and potential issues there.</p>


</section>

 ]]></description>
  <category>pharmaverse</category>
  <category>data</category>
  <guid>https://brianrepko.github.io/blog/posts/2026-01-15-datasetjson-part2/</guid>
  <pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2026-01-15-datasetjson-part2/datasetjson.png" medium="image" type="image/png" height="72" width="144"/>
</item>
<item>
  <title>Dataset-JSON</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2025-12-31-datasetjson-part1/</link>
  <description><![CDATA[ 





<p>As part of R/Pharma this year, I attended a <a href="https://atorus-research.github.io/datasetjson_workshop/" target="_blank">workshop</a> on the CDISC Dataset-JSON standard. This was a great workshop and props/kudos/flowers to all the folks that put that together. More recently, CDISC <a href="https://mailchi.mp/cdisc.org/2025bod-12872980?e=d16406719d" target="_blank">announced</a> that v1.0 of the API standard is released as well as the supplementary details on compression (aka Compressed Dataset-JSON v1.1).</p>
<p>This later announcement (and some a review of Pilot 5 updates) gives me a chance to dive into this important set of specifications and updates - thus this series.</p>
<p>I‚Äôm planning to do an overview in this part 1, go over some potential issues and clarifications in part 2, and then go over the REST API in part 3 (which TBH I‚Äôve not looked at in detail yet, so it goes last).</p>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>The data sent to regulatory authorities (eg. FDA for USA, EMA for EU, PMDA for Japan, etc.) typically conforms to <a href="https://www.cdisc.org" target="_blank">CDISC</a> international standards. When transferring this (tabular) data - from sponsor to RA - the <a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000464.shtml" target="_blank">SAS V5 XPORT format</a> (aka ‚ÄúXPT‚Äù) is currently used (required for the FDA). XPT was defined in 1989 and its use by the FDA was made official in 1999. It has many disadvantages - some of which we will highlight below.</p>
<p>Also in 1999, CDISC created the Operational Data Model (ODM). ODM is a foundational data standard that underlies many of the other CDISC standards and is XML-based. Between 2000 and 2005, ODM versions 1.0 through 1.3 were released. Some point releases, ODM v1.3.1 and v1.3.2 were released in 2010 and 2013 respectively. So from 1999 to 2013, data was defined based on ODM XML foundational standards and still transferred in XPT.</p>
<p>As industry is exploring coding alternatives to SAS - and potentially with sponsors creating all submission materials with R (or without SAS) - other data transfer format alternatives are needed. Moving to new data transfer standards will also allow CDISC to move beyond the limitations imposed by the XPT format.</p>
<p>Given that ODM was XML-based, in 2012, work was done to create Dataset-XML, with version 1.0 released in 2014. This is a XML-based data transfer format based on ODM v1.3.2. However, it was found that Dataset-XML created data files that were quite large (already an issue for XPT) and it was not widely adopted by industry.</p>
<p>Skip ahead ten years, and in 2023, ODM v2.0 was released. Dataset-XML v1.0 was not updated - it is still an extension of the ODM XML schema (as far as I can tell with a cursory review) and may not need an update.</p>
<p>As part of the ODM v2.0 work, on the data transfer side, Dataset-JSON v1.0 was also released in 2023. One of the best articles on the need for, development of, and plans for Dataset-JSON is <a href="https://www.clinicalleader.com/doc/no-more-xpt-piloting-new-dataset-json-for-fda-submissions-0001" target="_blank">this one from Sam Hume</a>.</p>
<p>A number of hackathons and pilot projects were done - some issues found - and then Dataset-JSON v1.1 was released in Dec 2024 to address those issues. Some issues required changes to the specification - some were addressed in the User Guide. <a href="https://phuse.s3.eu-central-1.amazonaws.com/Deliverables/Optimizing+the+Use+of+Data+Standards/WP-88+Dataset-JSON+Report.pdf" target="_blank">Here</a> is the report, from PHUSE, of those pilot projects.</p>
<p>Lastly, as mentioned above, the Dataset-JSON API v1.0 and Compressed Dataset-JSON v1.1 standards were released a few weeks ago in Dec 2025.</p>
</section>
<section id="dataset-json" class="level2">
<h2 class="anchored" data-anchor-id="dataset-json">Dataset-JSON</h2>
<p>The Dataset-JSON work actually has multiple components - with a view towards transfer of data via APIs instead of via files. In particular, this could become part of how sponsors can collect data from various other systems and collaborators - EDCs, CROs, etc.</p>
<p>These components are:</p>
<ul>
<li>Dataset-JSON specification and schema (<a href="https://github.com/cdisc-org/DataExchange-DatasetJson" target="_blank">github repository</a>)
<ul>
<li>Dataset-JSON v1.1</li>
<li>The NDJSON representation of Dataset-JSON</li>
<li>Compressed Dataset-JSON v1.1 (aka DSJC)</li>
<li>Dataset-JSON schema</li>
<li>User‚Äôs Guide (<a href="https://wiki.cdisc.org/display/PUB/Dataset-JSON+v1.1+User%27s+Guide" target="_blank">cdisc wiki</a>)</li>
</ul></li>
</ul>
<p>and</p>
<ul>
<li>Dataset-JSON API (<a href="https://github.com/cdisc-org/DataExchange-DatasetJson-API" target="_blank">github repository</a>)
<ul>
<li>Specification as HTML or OpenAPI</li>
<li>User‚Äôs Guide</li>
</ul></li>
</ul>
</section>
<section id="issues-with-xpt" class="level2">
<h2 class="anchored" data-anchor-id="issues-with-xpt">Issues with XPT</h2>
<p>Anyone that has looked at clinical trial data has wondered why column names are limited to 8 characters - it‚Äôs XPT.</p>
<p>The data is always tabular (rows and columns). Columns have name, label / description, type, and length and formatting options. There is no row identifier (unlike R data.frames) - access, <em>in SAS</em>, is via row number (<code>POINT=</code>) or via indices defined on top of the data (<code>KEY=</code>).</p>
<p>Here is a quick list of issues with XPT v5:</p>
<ul>
<li>Column/variable types - only CHARACTER (string) and DOUBLE (numeric - integer or floating point)</li>
<li>Column names are limited to 8 alphanumeric + <code>_</code> characters</li>
<li>Column labels are limited to 40 characters</li>
<li>Character values are US ASCII only with max length of 200 characters/bytes</li>
<li>Character values are stored with padding (so, larger than they need to be)</li>
<li>Numeric values are stored as IBM hexidecimal floating point (aka HFP, aka IBM-style double) format
<ul>
<li>Which is <strong>NOT</strong> IEEE 754, see <a href="https://en.wikipedia.org/wiki/IBM_hexadecimal_floating-point" target="_blank">wikipedia</a></li>
<li>This is why XPT is technically a binary file format - numeric data encoding in the file</li>
</ul></li>
<li>Inability to compress files - leads to data set splitting</li>
<li>There is no internally stored metadata
<ul>
<li>eg. file metadata, formatting on numerics, padding for characters, date/time formatting, keys</li>
</ul></li>
</ul>
<p>Note that modern versions of SAS use IEEE 754 internally for floating point data - it is only the XPT format that uses the HFP format. Also note that SAS does support date, time, and datetime variables - these are internally stored as numbers - with datetime point 0 being Jan 1, 1960 00:00:00 UTC. Actually, I have no idea how timezones work in SAS (all data is UTC and timezone is a system option?).</p>
<p>There is an XPT v8 format as well which made the following changes:</p>
<ul>
<li>Column names are extended to 32 characters (case sensitive)</li>
<li>Column labels are extended to 256 bytes</li>
<li>Character values are extended 32767 bytes</li>
<li>It is not clear if US ASCII is still a limitation or not (note the use of bytes vs character limits) - seems like it is limited to US ASCII</li>
</ul>
<p>In the end, there was not much industry uptake of XPT v8 and efforts were put elsewhere.</p>
</section>
<section id="quick-overview-of-dataset-json" class="level2">
<h2 class="anchored" data-anchor-id="quick-overview-of-dataset-json">Quick overview of Dataset-JSON</h2>
<p>Dataset-JSON is similar to XPT in that it is a single-file for tabular data.</p>
<p>As JSON (<a href="https://cdisc-org.github.io/DataExchange-DatasetJson/doc/dataset-json1-1.html" target="_blank">link to html version of spec here</a>), a dataset is a single object. Dataset metadata are attributes in that object, there is a <code>columns</code> array for column definitions, and then a <code>rows</code> array-of-arrays for data values.</p>
<p>Column definitions have the following attributes - unique ID (<code>itemOID</code>), <code>name</code>, <code>label</code>, <code>dataType</code>, <code>targetDataType</code>, <code>length</code>, <code>displayFormat</code>, and <code>keySequence</code>.</p>
<p>Note that <code>dataType</code> is the ‚Äúlogical‚Äù data type - tied to ODM - with values of <code>string</code>, <code>integer</code>, <code>decimal</code>, <code>float</code>, <code>double</code>, <code>boolean</code>, <code>datetime</code>, <code>date</code>, <code>time</code>, and <code>URI</code>.</p>
<p>The column attribute <code>targetDataType</code> is used for some logical types (decimals as strings, date/time/datetime as integers). We‚Äôll discuss this more in part 2. It looks like boolean values do use JSON <code>true</code> and <code>false</code> (in other places I‚Äôve seen these not used in favor of ‚ÄúY‚Äù,‚ÄúN‚Äù strings). Missing values are represented with JSON <code>null</code>. The empty string can be a value.</p>
<p>For the NDJSON representation - it is the same as above with the following changes:</p>
<ul>
<li>Each line is a JSON object</li>
<li>Row 1 is a JSON object that is all metadata and column array</li>
<li>Row 2-n are each an array of data
<ul>
<li>This is basically the <code>rows</code> array with ‚Äúlines‚Äù being entries in the array-of-array</li>
</ul></li>
</ul>
<p>NDJSON is very useful for streaming large sets of data.</p>
</section>
<section id="next-article" class="level2">
<h2 class="anchored" data-anchor-id="next-article">Next article</h2>
<p>In part 2, I‚Äôll make a list of some of the issues that come up in reading the specification and how some of the issues found in the pilot were handled.</p>
<p>Overall, I‚Äôm thrilled with this effort as it opens up clinical trial data to any programming language AND data transfer via API - but some details need some discussion. My intent is to highlight and/or clarify points related to data representation (from someone with a whole 30 year career in data engineering) and not to only criticize.</p>
<p>I hope those discussions will get added to the User‚Äôs Guide or maybe more <a href="https://pharmaverse.github.io/blog/" target="_blank">pharmaverse blog</a> posts (like <a href="https://pharmaverse.github.io/blog/posts/2023-10-30_floating_point/floating_point.html" target="_blank">here</a>, <a href="https://pharmaverse.github.io/blog/posts/2023-09-26_date_functions_and_imputation/date_functions_and_imputation.html" target="_blank">here</a>, and <a href="https://pharmaverse.github.io/blog/posts/2023-07-24_rounding/rounding.html" target="_blank">here</a>).</p>


</section>

 ]]></description>
  <category>pharmaverse</category>
  <category>data</category>
  <guid>https://brianrepko.github.io/blog/posts/2025-12-31-datasetjson-part1/</guid>
  <pubDate>Wed, 31 Dec 2025 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2025-12-31-datasetjson-part1/datasetjson.png" medium="image" type="image/png" height="72" width="144"/>
</item>
<item>
  <title>TileDB and Snowflake integration</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2025-12-20-tiledb-snowflake/</link>
  <description><![CDATA[ 





<p>I attended a webinar / demo of TileDB Carrara and it‚Äôs integration with Snowflake a few weeks ago. You can find the <a href="https://www.youtube.com/watch?v=cwcUdzkhVm0" target="_blank">recording on YouTube</a>.</p>
<p>I am familiar with TileDB as a tensor (aka n-dimensional) data format - a format often used for biomedical data. In fact, they have special formats, beyond general Arrays, for single-cell, VCF, and image data. You can find details at the <a href="https://cloud.tiledb.com/academy/structure/life-sciences/index.html" target="_blank">TileDB Academy website</a>.</p>
<p>But Carrara was new to me - it is a combination of ‚Äúdata‚Äù catalog (vs files) for a given project but also includes a notebook / compute environment. I found this <a href="https://www.youtube.com/watch?v=ic8EhcRStq0" target="_blank">short demo on YouTube</a> as a good overview of Carrara. What was cool to see was how these special formats can be rendered directly in the tool but also the notion of multi-file datasets rendered as a single entry in the project.</p>
<p>For the integration, you can see your Snowflake-based tabular data in your Carrara environment. And you could also see your TileDB-based multi-dimensional data in your Snowflake data - as tabular data. This basically allows you to merge multi-dimensional and tabular data with notebooks. On the Snowflake side, this could then be used for models or other computation that is added to your Snowflake environment.</p>
<p>This made me wonder if something similar wasn‚Äôt possible for DuckDB - could I see multi-dimensional TileDB data in DuckDB as tables? This doesn‚Äôt seem to be supported and, for me as an open source advocate, solidifies the need for open source alternatives to TileDB (eg. hdf5, Zarr, COG). I‚Äôll have a later post on Zarr vs hdf5 (and more) since hdf5 doesn‚Äôt work so great on cloud-based storage.</p>
<p>That said, the ability to integrate TileDB multi-dimensional data and Snowflake together is a great expansion for multi-omic data analysis.</p>



 ]]></description>
  <category>bioinformatics</category>
  <category>data</category>
  <guid>https://brianrepko.github.io/blog/posts/2025-12-20-tiledb-snowflake/</guid>
  <pubDate>Sat, 20 Dec 2025 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2025-12-20-tiledb-snowflake/logos-snowflake-tiledb.png" medium="image" type="image/png" height="56" width="144"/>
</item>
<item>
  <title>Validation of R libraries for FDA/EMA submissions</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2025-11-06-quarto-gherkin/</link>
  <description><![CDATA[ 





<p>Since R/Pharma 2023 in Chicago, I‚Äôve wanted to look into using the techniques of ATDD (or BDD‚Ä¶ or Specification by Example) for testing sets of R packages (aka - an R library). This was a topic that came up as part of the <a href="https://pharmar.org" target="_blank">R Validation Hub</a> work that was presented at that conference. Particularly around the discussion on how can multiple pharma companies share their R library validation tests. It was a chance to bring a technique from software engineering that I was familiar with (in Java and Python) to the world of R and in particular the <a href="https://pharmaverse.org" target="_blank">pharmaverse</a>.</p>
<p>It‚Äôs now 2 years later and I will have a pre-recorded talk at the (<strong>free!</strong>) virtual <a href="https://rinpharma.com" target="_blank">R/Pharma</a> 2025 conference today (Nov 6, 2025). R/Pharma makes all the sessions available in about a month on <a href="https://www.youtube.com/rinpharma" target="_blank">YouTube</a> - this is a community that shares and grows together after all - but here are the early links to where I host things.</p>
<ul>
<li>The pre-recorded video is hosted <a href="https://www.learnthinkcode.com/files/rinpharma-2025/rinpharma-2025-atdd.mp4" target="_blank">here</a></li>
<li>The slides are hosted <a href="https://www.learnthinkcode.com/files/rinpharma-2025/presentation.html" target="_blank">here</a></li>
</ul>
<p>While putting this together, I found <a href="https://gojko.net/2020/03/17/sbe-10-years.html" target="_blank">this link</a> from Gojko Adzic, author of ‚ÄúSpecification by Example‚Äù on where this technique sits in our bag of software engineering tools after 10 years - now 15 years - later. It validates that software quality is enhanced by the use of this technique - even if used for requirements only - versus being used for both requirements and automated testing. It also validates that the use of the Gherkin-based frameworks - Given/When/Then - are the norm - and fortunately, for R, we now have Jakub Sobolewski‚Äôs <a href="https://jakubsobolewski.com/cucumber/" target="_blank"><code>{cucumber}</code></a> package.</p>
<p>The more I think about what this could look like - and potentially integrated into Quarto - I see the following:</p>
<ul>
<li>The ‚Äúliving documentation‚Äù is a Quarto website with potentially additional descriptive explanation around gherkin text (along with <code>sessionInfo()</code> output)</li>
<li>The website can be organized into pages - Quarto documents with <code>```gherkin</code> code-blocks that contain the actual gherkin text</li>
<li>Quarto could be configured with a gherkin engine - for R, that could be <code>knitr+cucumber</code> (and can imagine a python engine as well)</li>
<li>Quarto rendering invokes knitr and cucumber to execute the gherkin tests and formats the output for <code>pandoc</code> to render the results as part of the report</li>
<li>Steps could be registered on the Quarto page in an <code>```{r}</code> block or in a package and discovered on package load</li>
<li>The report could be included as part of the validation report</li>
</ul>
<p>I‚Äôm super interested in people‚Äôs thoughts on this - please reach out via links at the end of the slides.</p>



 ]]></description>
  <category>pharmaverse</category>
  <category>agile</category>
  <guid>https://brianrepko.github.io/blog/posts/2025-11-06-quarto-gherkin/</guid>
  <pubDate>Thu, 06 Nov 2025 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2025-11-06-quarto-gherkin/logo-rinpharma.png" medium="image" type="image/png" height="50" width="144"/>
</item>
<item>
  <title>Blogging again‚Ä¶for real</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2025-10-31-blogging-for-real/</link>
  <description><![CDATA[ 





<p>The <a href="../2014-09-28-blogging-again-with-a-change-of-focus/">2014 post</a> that I wrote was about my move into bioinformatics as well as a move to Switzerland. Specifically, working in the Oncology disease area of Novartis Biomedical Research.</p>
<p>The <a href="../2019-04-09-snowflake-for-biomedical-research/">2019 post</a> that I wrote had me back in Minneapolis and working for Carrot Health. Still related to health - data engineering for predictive models in health care (social determinants of health, healthcare quality, etc.).</p>
<p>So, you can tell I did <em>NOT</em> start blogging again - and now it‚Äôs been 6 years.</p>
<p>Based on COVID work protocols, Novartis allowed for remote work in the disease areas and a position opened up with the Oncology team. I was fortunate to return in Oct 2020. I was even more fortunate to be able to retire, just before my 60th birthday, in June 2025.</p>
<p>Thus I can say that I truly will be blogging more - for real. I just converted my old Wordpress blog into this Quarto-based one.</p>
<p>And I have a bunch of projects - some are based in Minneapolis. I‚Äôm serving as the Twin Cities city captain for <a href="https://www.bitsinbio.org">Bits-in-Bio</a>. I‚Äôm also spending more time with <a href="https://www.isaiahmn.org">ISAIAH</a> - a pro-democracy multi-racial, multi-faith group here in Minnesota.</p>
<p>On the bioinformatics side, there are also a ton of projects.</p>
<ul>
<li>A system for bringing <a href="https://en.wikipedia.org/wiki/Specification_by_example">Specification by Example</a> to R libraries - starting with the <a href="https://pharmaverse.org">pharmaverse</a></li>
<li>A system for collecting genome and gene references in parquet / duckdb</li>
<li>Single-cell / Spatial dataset conversion with duckdb (using the hdf5 extension)</li>
<li>Improvements to the OMOP CDM data model</li>
<li>VCFs at scale (potentially with Zarr)</li>
<li>Potentially some algorithms using simplicial topology with multi-omic datasets (spatial transcriptomics + H&amp;E stain images)</li>
<li>and lots more (literally, I have a page of potential projects)</li>
</ul>
<p>Here in Minnesota, residents at age 62 can attend classes at the UMN for free. In a few years, I‚Äôll be going back to school but I‚Äôm not sure what classes I‚Äôll be pursuing.</p>
<p>In short, stay tuned - there will be more getting published here. I‚Äôll also add notifications to new blog posts on Mastodon and LinkedIn - links you can find at the top of this page.</p>



 ]]></description>
  <category>bioinformatics</category>
  <guid>https://brianrepko.github.io/blog/posts/2025-10-31-blogging-for-real/</guid>
  <pubDate>Fri, 31 Oct 2025 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2025-10-31-blogging-for-real/blogging.png" medium="image" type="image/png" height="123" width="144"/>
</item>
<item>
  <title>Snowflake for Biomedical Research</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2019-04-09-snowflake-for-biomedical-research/</link>
  <description><![CDATA[ 





<p>I‚Äôve since left biomedical research (the Genomics team at Novartis Institutes for Biomedical Research - NIBR) and am now doing health analytics with Carrot Health. At Carrot Health, we are making use of Snowflake Computing as our data storage and query system. I love Snowflake and there are so many features that make it better than what we had at NIBR. In the spirit of <a href="https://www.snowflake.com/blog/top-10-cool-things-i-like-about-snowflake/">‚ÄúTop 10 Cool Things I Like About Snowflake‚Äù,</a> I bring you my 10 reasons that Snowflake works for biomedical research.</p>
<p>Reason 1 - All in the Cloud (AWS or Azure) - no DBA, no hardware, no tuning.</p>
<p>You can be up and running instantly in your cloud of choice. There are even safety features like UNDROP, should you DROP a table or view by mistake. For bio folks - if you are a small lab and don‚Äôt have DBAs and hardware admins, etc., then no problem. And if you do, then the security features (below) should be enough to get your IT department to buy-in.</p>
<p>Reason 2 - Persistent (cached) results (with a visual execution plan)</p>
<p>In Snowflake, if you (or anyone) executes a query with a result set, that result is cached and used either in other query plans or as a direct result with the same SQL. This might mean that the first time you execute a query it might need some horsepower, but then for 24 hours after that, you‚Äôll get that data back instantly. This is great for bio folks that are typically working on a particular project‚Äôs data - joined with large public datasets. Those can be cached for a while and then when you are on to the next project, they will just get removed from cache. And if you aren‚Äôt sure, you can easily go into Query History and look at the visual execution plan.</p>
<p>Reason 3 - The functions you need - pivot / unpivot, analytic functions and UDFs</p>
<p>Let‚Äôs face it - bio folks like their data in matrix form sometimes - pivot and unpivot in the database is great. Having the ability to do a wide variety of analytic functions can help with basic statistics. And having the ability to add your own functions is great too - but ECMAscript only.</p>
<p>Reason 4 - JSON in SQL</p>
<p>Snowflake supports a VARIANT column type that can hold JSON data and it has the SQL extensions to query that data. This is super useful for mixing structured and semi-structured data together. And that is key for aggregating bio data - because we can almost agree on most of the structure but then everyone has their extra data that they want to keep.</p>
<p>Reason 5 - Can connect from anything</p>
<p>Snowflake supports ODBC, JDBC, Python, Spark, a web console, and it‚Äôs own snowsql command. You can basically use any tool to get connected. We were able to easily add support for SchemaSpy and Flyway (JDBC-based tools) for Snowflake - and I typically use DbVisualizer (JDBC) to access it.</p>
<p>Reason 6 - Like a data lake but with SQL</p>
<p>Snowflake has amazing capabilities to both load and unload data to and from S3 (we are in AWS and now Azure) and it‚Äôs fast. You can regularly point it at a folder and it knows which files have already been loaded. And you can define that process to happen automagically. There are some file formats that it doesn‚Äôt support out of the box - I‚Äôve had to convert some fixed width data to separated-values - but that is minor compared to the built-in infrastructure. For bio folks, I think that this is awesome - getting scientists to put their data in S3 is far easier than helping them get it into the database.</p>
<p>Reason 7 - Data Sharing, Data Cloning, and Time Travel</p>
<p>Snowflake has the ability to share databases between accounts. This means that someday, we could have reference data already loaded (once) in Snowflake and have everyone share it. Or results of a consortium‚Äôs work could be shared once. Sharing WITH SQL access. There is also the ability to quickly clone data which is another way that one can share data / parts of data (or promote data from QA to PROD). Snowflake, like Datomic, also has the ability to return results based on the data at a given time. For bio folks, this is exactly what is needed for reproducible research - and/or - for data that changes over time but you don‚Äôt want to deal with formal versioning.</p>
<p>Reason 8 - Multiple Databases and Schemas</p>
<p>Snowflake is one of the few systems that supports multiple databases with multiple schemas in them. And all SQL can cross databases and schemas. This helps tremendously with data organization and potentially with role-based sharing rights. And security doesn‚Äôt stop there - data is encrypted at rest and in transit and you can even lock down access to your own AWS PrivateLink so traffic never leaves your combined data center / AWS cloud. Snowflake is HIPAA and SOC2 compliant as well.</p>
<p>Reason 9 - Scaling compute vs scaling storage</p>
<p>With Snowflake, the SQL execution ‚Äúcluster‚Äù is called a ‚Äúwarehouse‚Äù (horrible name, I know, but there you are). One can size (and resize) a warehouse for the queries at hand - thus having the ability to scale at will as needed (there are even warehouse clusters to get you even more compute if you need). You pay separately for storage and compute but you have tremendous control over it (and access to the accounting). You can even has department only warehouses to enable chargeback policies.</p>
<p>Reason 10 - the bleeding edge is available</p>
<p>Snowflake supports parquet files as well. It would be awesome to try and use ADAM-formatted data - or heck, run a whole Big Data Genomics variant calling pipeline directly on the database. Or could be fun to try a version of hail.is directly on the database. This is something I‚Äôd love to see people try - and is only do-able in Snowflake.</p>
<p>So there are my 10 - please feel free comment or email me at brian -dot- repko -at- learnthinkcode -dot- com. I should add that I‚Äôm writing this as a way to share my experience with my past co-workers and Snowflake has not asked for this or is supporting this in anyway. My thoughts and opinions only.</p>



 ]]></description>
  <category>architecture</category>
  <category>bioinformatics</category>
  <guid>https://brianrepko.github.io/blog/posts/2019-04-09-snowflake-for-biomedical-research/</guid>
  <pubDate>Tue, 09 Apr 2019 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2019-04-09-snowflake-for-biomedical-research/snowflake-logo.png" medium="image" type="image/png" height="34" width="144"/>
</item>
<item>
  <title>Blogging again</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2014-09-28-blogging-again-with-a-change-of-focus/</link>
  <description><![CDATA[ 





<p>A few years back, I noticed a friend‚Äôs LinkedIn update that he was working for Entagen, a company doing life sciences work in the Twin Cities. And I knew that I had to jump at the chance to get into life science programming. I was subcontracted to do some work at Novartis - the research branch in particular (NIBR) and then quickly found myself working on a project to manage and merge all public genomic data. I‚Äôm so thankful to the people that made that all happen.</p>
<p>And I‚Äôve never looked back. It‚Äôs been 3 years, a move to a new country (Switzerland) and a new disease area (Oncology) and I love my work. I love learning the science and seeing where good software engineering can make a difference. If ‚Äúbioinformatician‚Äù means a ‚Äúsoftware engineer that uses their knowledge of biology‚Äù - then I now call myself a bioinformatician.</p>
<p>It‚Äôs been a while since I was blogging - but I‚Äôm going to start up again. However, the blog will be more focused on the state of software engineering in the life sciences (from my one perspective) and where technology is going in this space. Stay tuned‚Ä¶</p>



 ]]></description>
  <category>bioinformatics</category>
  <guid>https://brianrepko.github.io/blog/posts/2014-09-28-blogging-again-with-a-change-of-focus/</guid>
  <pubDate>Sun, 28 Sep 2014 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2014-09-28-blogging-again-with-a-change-of-focus/blogging.png" medium="image" type="image/png" height="123" width="144"/>
</item>
<item>
  <title>Slides from Practical Agility</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-09-22-slides-from-practical-agility-jbehave-and-fit-goodbadugly/</link>
  <description><![CDATA[ 





<p>I presented a ‚ÄúLightning Talk‚Äù (6 minutes) at the last Practical Agility meeting on ‚ÄúJBehave and FIT - the Good, the Bad and the Ugly‚Äù. For the talk everything was on NEON cards (neon-green, neon-yellow and neon-red! - its all I could find at Walgreens) and before throwing them out thought that I would put them into powerpoint (minus the neon) and share. Slides are up on <a href="http://www.slideshare.net/brianrepko/fit-and-j-behave">SlideShare</a> - but you might want to download as all the notes are in the notes section. Enjoy!</p>



 ]]></description>
  <category>agile</category>
  <category>java</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-09-22-slides-from-practical-agility-jbehave-and-fit-goodbadugly/</guid>
  <pubDate>Wed, 22 Sep 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-09-22-slides-from-practical-agility-jbehave-and-fit-goodbadugly/talk-2.png" medium="image" type="image/png" height="98" width="144"/>
</item>
<item>
  <title>JBehave 3.0 released!!</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-09-02-jbehave-3-0-released/</link>
  <description><![CDATA[ 





<p>JBehave 3.0 was <a href="http://jbehave.org/2010/08/31/jbehave-3-0-released/">released</a> yesterday (finally!!). I‚Äôm thrilled to have donated the Multi-tennant Spring Security example (which has been updated to JBehave 3). That example is now part of the many examples that are included in JBehave. Looking to update the presentation on my website to explain some of the new features that make up JBehave 3.0. Congrats to Mauro and Paul on all their hard work!</p>



 ]]></description>
  <category>agile</category>
  <category>java</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-09-02-jbehave-3-0-released/</guid>
  <pubDate>Thu, 02 Sep 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-09-02-jbehave-3-0-released/jbehave-logo.png" medium="image" type="image/png" height="58" width="144"/>
</item>
<item>
  <title>Extending Jasypt - AES and Blowfish support</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-07-21-extending-jasypt-aes-and-blowfish-support/</link>
  <description><![CDATA[ 





<p>I recently had to code Java / Perl interoperable encryption - in Perl it was using the <code>Crypt::CBC</code> and <code>Crypt::Blowfish</code> modules. The perl code was meant to be as simple as possible:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode perl code-with-copy"><code class="sourceCode perl"><span id="cb1-1"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">$cipher</span> = <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">Crypt::CBC</span>-&gt;new( -cipher =&gt; <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">'</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">Blowfish</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">'</span>, -key =&gt; <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">'</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">password</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">'</span> ); </span>
<span id="cb1-2"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">$ciphertext</span> = <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">$cipher</span>-&gt;<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">encrypt_hex</span>(<span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">"</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">This data is super secret hush hush</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">"</span>);</span></code></pre></div></div>
<p>The key is really a passphrase that is then generated into a key and IV for use with the underlying CBC/cipher. These modules are by default compatible with OpenSSL. I thought that since this is password-based encryption, that I could use Jasypt, one of my favorite libraries. Unfortunately, Jasypt only supports the PBE* cipher algorithms and none of them are the OpenSSL standards. So then I thought that I could at least get Jasypt to support Blowfish. No luck‚Ä¶the algorithm is just hard-coded to PBE-based encryption. Even the IV work would be impossible.</p>
<p>So for the project, I created my own mini-framework that includes converters (hex/base64 string to byte[] or String to byte[] based on a character-set) and ciphers defined via generics with the ability to create a string-to-string ‚Äúencryptor‚Äù by combining a String-to-byte[] (utf-8) converter with a byte[]-to-byte[] cipher with a byte[]-to-String (hex) converter. This is sort of what Jasypt does but its not very pluggable in that fashion.</p>
<p>Then to write the byte[]-to-byte[] cipher, I started with a generalized algorithm that works for both the PBE* algorithms but also for AES and Blowfish with the key and IV generation handled in the process. Plug in BouncyCastle‚Äôs <code>OpenSSLPBEParametersGenerator</code> for key/IV generation and write my own decorator for dealing with sharing the salt as ‚ÄúSalted__XXXXXXXX‚Äù in front of the ciphertext and voila! Perl-Java encryption interoperability based on passwords and random salts!!</p>
<p>That project has ended and I‚Äôm now in-between gigs so I worked that code into Jasypt - just added a feature request (with a patch) to Jasypt. Not specifically the perl stuff but the generalized algorithm. That allows users to finally extend Jasypt - still for password-based encryption but not limited to the PBE* algorithms. Support is finally in there for AES and Blowfish with key and IV generation based on PBKDF2 or whatever else you want to add. Changes to Jasypt to support the configuration of the whole ‚Äúpipeline‚Äù is not in there - that would require some serious changes to Jasypt.</p>
<p>As for the algorithms - they look like this:</p>
<p>For encryption,</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode java code-with-copy"><code class="sourceCode java"><span id="cb2-1">EncryptionData data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">buildEncryptionData</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">();</span></span>
<span id="cb2-2">data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">setMethodInput</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>message<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> </span>
<span id="cb2-3">dataProcessor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">preProcess</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Cipher</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ENCRYPT_MODE</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> </span>
<span id="cb2-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">SecretKey</span> key <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> keyGenerator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">generateSecretKey</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> </span>
<span id="cb2-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">AlgorithmParameterSpec</span> parameterSpec <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> paramGenerator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">generateParameterSpec</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">synchronized</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>encryptCipher<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span> </span>
<span id="cb2-7">  encryptCipher<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">init</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Cipher</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ENCRYPT_MODE</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> parameterSpec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> </span>
<span id="cb2-8">  data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">setCipherOutput</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>encryptCipher<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">doFinal</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">getCipherInput</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">()));</span> </span>
<span id="cb2-9"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb2-10">dataProcessor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">postProcess</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Cipher</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ENCRYPT_MODE</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb2-11"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">getMethodOutput</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">();</span></span></code></pre></div></div>
<p>For decryption,</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode java code-with-copy"><code class="sourceCode java"><span id="cb3-1">EncryptionData data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">buildEncryptionData</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">();</span> </span>
<span id="cb3-2">data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">setMethodInput</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>encryptedMessage<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> </span>
<span id="cb3-3">dataProcessor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">preProcess</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Cipher</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">DECRYPT_MODE</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> </span>
<span id="cb3-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">SecretKey</span> key <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> keyGenerator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">generateSecretKey</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> </span>
<span id="cb3-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">AlgorithmParameterSpec</span> parameterSpec <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> paramGenerator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">generateParameterSpec</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> </span>
<span id="cb3-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">synchronized</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>decryptCipher<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span> </span>
<span id="cb3-7">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">this</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">decryptCipher</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">init</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Cipher</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">DECRYPT_MODE</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> parameterSpec<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> </span>
<span id="cb3-8">  data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">setCipherOutput</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>decryptCipher<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">doFinal</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span>data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">getCipherInput</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">()));</span> </span>
<span id="cb3-9"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span> </span>
<span id="cb3-10">dataProcessor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">postProcess</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Cipher</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">DECRYPT_MODE</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span> </span>
<span id="cb3-11"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">getMethodOutput</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">();</span></span></code></pre></div></div>
<p>And all the real work is done in the <code>SecretKeyGenerator</code> (which actually generates more than just the key - it just returns the key), the <code>AlgorithmParamsGenerator</code> and the <code>EncryptionDataProcessor</code> - all of which are just interfaces. All the transient data for the method is kept in the <code>EncryptionData</code> class or subclass. So that is the patch just submitted.</p>
<p>And perl interoperability could be added with an <code>OpenSSLSecretKeyGenerator</code> and an <code>OpenSSLEncryptionDataProcessor</code> to handle the ‚ÄúSalted__XXXXXXX‚Äù format - the rest is all in there (once the patch is approved, committed and released). The <code>OpenSSLSecretKeyGenerator</code> would work like the <code>PBKDF2SecretKeyGenerator</code> in that it would produce the key and IV based on a password and fixed or random salt. Its just that OpenSSL does a funky key and IV generation mechanism that I‚Äôm not sure is in the default JCE providers. And the <code>OpenSSLEncryptionDataProcessor</code> is just an extension of the existing one with the hardcoded ‚ÄúSalted__‚Äù thrown in for good measure.</p>
<p>Here‚Äôs hoping that gets added by the next project. Jasypt team - I‚Äôm more than willing to help!</p>



 ]]></description>
  <category>java</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-07-21-extending-jasypt-aes-and-blowfish-support/</guid>
  <pubDate>Wed, 21 Jul 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-07-21-extending-jasypt-aes-and-blowfish-support/encryption-9.png" medium="image" type="image/png" height="128" width="128"/>
</item>
<item>
  <title>JBehave presentation for Twin Cities JUG</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-04-12-jbehave-presentation-for-twin-cities-java-users-group/</link>
  <description><![CDATA[ 





<p>Finally finished the JBehave presentation for tonight‚Äôs Twin Cities Java Users Group. You can find the PPT and source code at the LearnThinkCode <a href="http://www.learnthinkcode.com">website</a>. Any and all feedback is welcome.</p>
<p>In the end, I really think that this is the year that Agile testing via executable requirements will take off and I do think that JBehave can be a part of that. There are some key things to work out however (library bundling issues, integration with JUnit for Spring) that need to be looked at before its really ready for prime time. And getting the Pico Ajax Email / Selenium example working was painful. Please don‚Äôt release versioned software that depends on snapshot releases of other code!</p>
<p>So, it would be usable on projects if you are willing to spend some time on getting your base class / infrastructure stuff setup. Personally, I like it more than FIT/Fitnesse.</p>



 ]]></description>
  <category>agile</category>
  <category>java</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-04-12-jbehave-presentation-for-twin-cities-java-users-group/</guid>
  <pubDate>Mon, 12 Apr 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-04-12-jbehave-presentation-for-twin-cities-java-users-group/talk-1.png" medium="image" type="image/png" height="65" width="144"/>
</item>
<item>
  <title>Extreme distributed scrum - daily standup</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-02-26-extreme-distributed-scrum-daily-standup/</link>
  <description><![CDATA[ 





<p>I‚Äôve worked on Scrum teams where 1/2 the team is in one location and 1/2 in another (both offshore and onshore) and every now and then we would use an IM conference in order to have a ‚Äústandup‚Äù (except that we are sitting and on IM). We tried video and phone conferencing as well but given the lag in the network as well as lack of equipment and network availability, IM seemed to just work better. IM allowed for give-and-take (with some lag <em>but a lag we were familiar with</em>) and was always available. In addition, IM allowed the conversation to be sent via email for later reading and sharing (to the 1/2 of the team that wasn‚Äôt in yet). Since then, I‚Äôve wondered about what technology tools one would need if the team was completely separated (think rock stars all working from home).</p>
<p>If all the team members were located in their own location - how would I set this up? The kicker here is the mess of timezones that might be in the mix. Obviously, I‚Äôd have a wiki and perhaps an agile PM (kanban/scrum) web app running somewhere that we could all access in our timezone. When folks are distributed and have an overlapping time, then can use VNC (or other free solutions like that) to ‚Äúpair-up‚Äù as needed. Likewise, VoIP/IM conferences or just VoIP/IM for issues and/or questions.</p>
<p>But how to do ‚Äústandups‚Äù when there isn‚Äôt a time that everyone can standup? How to let someone know you are stuck on something and how to hand off a potential solution to someone that will get it hours later. My insight was that a team could host an <strong>internal blog/Twitter</strong> to share what they did yesterday, what they are doing today and what issues are blocking them. Status updates (‚Äúworking on X‚Äù or ‚Äúcan‚Äôt figure out Y‚Äù) can then really be done at anytime and those folks that are online can help step in. Some IM systems have a status but I‚Äôm not sure that that is very visible. <strong>Add an RSS feed on top</strong> of the teams blogs (like twitter) and you‚Äôll start to see team collaboration. Start your day by reading all the updates from folks since you were last on. The whole project life could be read if you really wanted too - like emailing the IM discussion. Still doesn‚Äôt help with the I have an issue with X and what a potential solution might be (hours later). I could see the wiki or issue tracking system kind of work in that space. In this scenario, the world of ‚Äústandup‚Äù starts to look like ‚Äústatus‚Äù - but that might be ok just given the realities of multiple people in multiple timezones. I thought that it would be an interesting way to run a project without standups.</p>
<p>Iteration and release planning would be difficult in this situation - that might just have to be done together.</p>
<p>I‚Äôve not had this situation - have you? What worked or didn‚Äôt work?</p>



 ]]></description>
  <category>agile</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-02-26-extreme-distributed-scrum-daily-standup/</guid>
  <pubDate>Fri, 26 Feb 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-02-26-extreme-distributed-scrum-daily-standup/twitterrssfeed1-main_full.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Making Ant 1.8 work like Maven - not so much</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-02-19-making-ant-1-8-work-like-maven-not-so-much/</link>
  <description><![CDATA[ 





<p>I‚Äôve done Ant build files in the past that ended up working like Maven2. Mostly since it was a non-Maven shop but also because it was a way to get folks into Maven-think but by using Ant.</p>
<p>Now, <a href="http://ant.apache.org/">Ant 1.8</a> has been released and with it some new features that could potentially make it possible to have very modular Ant builds that would be even better than Maven2. One of the main concepts within Maven2 is the various <a href="http://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html">lifecycles</a> (clean, build/default and site) and that build tasks from plugins are bound to various parts of the lifecycle. Ant 1.8 introduced the notion of <a href="http://ant.apache.org/manual/targets.html">extension-points and extensionOf</a> as well as <a href="http://ant.apache.org/manual/CoreTasks/import.html">imports</a> and local properties - these could all be used to both create plugins (macrodefs) and our own lifecycles (sets of extension-points) and then bind them all up together in a build.xml and just import what you need - potentially from an http URL.</p>
<p>Well, that was the thought‚Ä¶</p>
<p>Turns out that imports are processed after the build.xml is parsed. That‚Äôs all well and good, but when an extensionOf attribute is parsed, Ant looks for the target(s) named in the extensionOf value in order to add the current target as a dependency. That requires that the target has to exist in the project and if that target is part of an import (as the documentation seems to suggest), then the target doesn‚Äôt exist (yet) at the time of parsing and you get a nice error message to that effect.</p>
<p>I think that this is a design flaw in how extension-point / extensionOf is supposed to work and contradicts the example cited in the documentation - which doesn‚Äôt work.</p>
<p>Its too bad because with these features, I could define my own lifecycles or even change/modify the existing ones from Maven2 to do things related to database SQL modules (create the database from all the SQL scripts and some data files) or be able to mix the SQL and java files together in the same module and add phases to the lifecycle related to database setup. This has always been something that I have to hack up the pom for anyway - which is part of why I like going back to Ant - I can change it easier when I need to.</p>
<p>Work-arounds? Change the ProjectHelper/TargetHelper to deal with extensionOf attributes after the import stack is popped (and all the targets are resolved) or import the extensionOfs (the bindings or which macros get called for each step) after the extension-points are imported. I‚Äôm not a fan of the latter as I really think that the bindings are the build - execute these steps for these lifecycle stages - but if my build is just a bunch of imports, that‚Äôs not the worst of it. Or screw the use of extension-points/extensionOf and just use imports with empty targets (which is kind of what extension-points are - except that I could then create a target that gets bound to multiple extension-points with extensionOf=‚Äútarget1,target2‚Äù).</p>
<p>It does sadden me that the example cited doesn‚Äôt even work however. If I get this working, I‚Äôll post the example.</p>



 ]]></description>
  <category>java</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-02-19-making-ant-1-8-work-like-maven-not-so-much/</guid>
  <pubDate>Fri, 19 Feb 2010 00:00:00 GMT</pubDate>
  <media:content url="https://www.apache.org/logos/res/ant/default.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Connecting Agile Teams - one pink post-it at a time</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-01-27-connecting-agile-teams-one-pink-post-it-at-a-time/</link>
  <description><![CDATA[ 





<p>I‚Äôve used color coded cards on Scrum boards - green for user story, blue for system story, yellow task cards for design or review work, blue task card for ‚Äútechnical architecture work‚Äù, etc. - lots of variations. Sometimes I suggest it, sometimes I don‚Äôt. Just part of the box of tools. The one thing that I‚Äôve noticed with this however was the use of pink cards and post-its and how they can be used to connect agile teams and help build an agile enterprise.</p>
<p>My original use of pink cards was for a Scrum board for developers to fix a critical or blocker bug. This was for a team that was just developers - the testers were a completely different team. And the pink card was basically a request for the development team to help un-block the testing team (and dev team would estimate it and decide if they would need to take something else off the board).</p>
<p>I‚Äôve also used pink post-its on Scrum boards to report a blocking issue on a task - just as a way to remind people working on the issue that it is important and to bring it up in standup until the issue is resolved.</p>
<p>On another team that I worked on we sort-of had a Kanban board for release or operations tasks related to the program (multiple projects - one operations team) and if the implementation team had a request to make of them (e.g.&nbsp;database to setup), then we would create a pink card for their board. Basically a pink card is a Please Do This ASAP request. Maybe pink should stand for Please Implement Now, Kind (Sir/Madam).</p>
<p>What I realized is that one way to connect these teams, with their own boards and tasks and stories is that <strong>the issue (post-it) is tied to request(s) to resolve the issue (cards) and you could track and connect those issues/tasks that way</strong>. So basically, for that first scenario (separate dev and test teams), if the test team had had a board, their pink post-it (the blocking issue) was tied to the pink card for the development team (the issue resolvers). And a board with a lot of pink is a conversation waiting to happen.</p>
<p>Simple and easy way to handle and track issues that need to get done now that I think helps build an agile enterprise.</p>



 ]]></description>
  <category>agile</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-01-27-connecting-agile-teams-one-pink-post-it-at-a-time/</guid>
  <pubDate>Wed, 27 Jan 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-01-27-connecting-agile-teams-one-pink-post-it-at-a-time/pink-post-it.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Scrum and Kanban together</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-01-27-scrum-and-kanban-together/</link>
  <description><![CDATA[ 





<p>One of my favorite links is Henrik Kniberg‚Äôs ‚Äú<a href="http://www.crisp.se/henrik.kniberg/Kanban-vs-Scrum.pdf">mini-book</a>‚Äù on Kanban and Scrum and how they work (and how they are similar and different). Given that description of Kanban and <a href="http://brianrepko.github.io/blog/posts/2010-01-25-when-is-a-story-prepared/">my thoughts on story prep and story release work</a>, I would really love to try a Kanban board for story prep and release work with a Scrum board for implementation work. Definitely for prep and implementation - release probably depends how that process looks - perhaps one release board for a program (multiple projects but one solution).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://brianrepko.github.io/blog/posts/2010-01-27-scrum-and-kanban-together/blog-kanban-scrum.jpg" class="quarto-figure quarto-figure-center figure-img" height="300"></p>
</figure>
</div>
<p>Backlog grooming would actively work the story prep board. The team could see what stories are getting ready for planning as well as a release team (or management team) seeing what is getting ready for release to production. I think that it would actually engage those team members that are at the daily standup but are not developers or testers - they can point to what they are working on - its just on the story prep kanban board. I think that it could make for a good information radiator for a wider ‚Äúteam‚Äù.</p>
<p>The other way to look at this, from a metrics standpoint, is to see that the whole Scrum board is just one column of a larger Kanban board and that you could measure and reduce the throughput time of a story from backlog to released to production on that larger Kanban board.</p>
<p>Has anyone ever done anything like this? Did it work? Things to improve about it?</p>
<p>Putting this post together, I just noticed Henrik‚Äôs <a href="https://www.infoq.com/minibooks/kanban-scrum-minibook/">Kanban and Scrum - Making the Most of Both</a> - something new to read!</p>



 ]]></description>
  <category>agile</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-01-27-scrum-and-kanban-together/</guid>
  <pubDate>Wed, 27 Jan 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-01-27-scrum-and-kanban-together/blog-kanban-scrum.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>When is a story prepared?</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-01-25-when-is-a-story-prepared/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://brianrepko.github.io/blog/posts/2010-01-25-when-is-a-story-prepared/blog-circles.jpg" class="quarto-figure quarto-figure-center figure-img" height="400"></p>
</figure>
</div>
<p>This is another drawing that I use a lot while coaching agile projects and actually is part of multiple discussions around agile methods. Most agile methods talk about stories being created by the customer and put on a backlog. For some iteration, at iteration planning, the story gets explained to developers and testers. They work on the story until it is complete. And we have lots of conversations, as agile coaches and teams, about ‚Äúwhen is a story complete‚Äù.</p>
<p>This misses a lot of the work that needs to be done in order to make teams effective. I like to have regular backlog grooming meetings with part of the team and ask the question - <em><strong>is this story prepared?</strong></em>. <em><strong>What is needed in order to bring this story to iteration planning?</strong></em> That needed work might involve QA (quality assurance) for acceptance tests or functional tests. That might involve UX (user experience) for wireframes or drawings. That might involve some TA (technical architecture) work or IA (information architecture, or domain modeling) work depending on the story. It might require a BA to work out the business value of this story or how to break it up into what needs to done now versus later (breaking up stories into smaller, potentially optional pieces). Its only when a story is prepared that it should be brought to iteration planning.</p>
<p>I also use this picture to help explain why some work is ‚Äúon the board‚Äù for the iteration (meaning we are tracking velocity and burndown charts - its the developer/tester circle) vs work that needs to get done but we aren‚Äôt measuring velocity for it. The first is working towards completing the story. The latter is working towards getting the story prepared.</p>
<p>It also helps explain the roles of the non-customer, non-developer and non-tester folks‚Ä¶though I‚Äôm pretty careful to explain that that 2nd circle is optional work (story by story) and that that work can be done by anyone with those skills. Its really about what would make the communication of this story effective and doing that in as lightweight of a fashion as you need.</p>
<p>The last part of this drawing is that the story doesn‚Äôt stop because development is done. Its really done when its deployed (some would say deployed to production) and supported. This means that the story needs to be shared with operations and support teams. I‚Äôve seen this done as part of a release process and actually made it the responsibility of the whole team to figure how to to communicate the stories that are being released. Really each circle needs to figure out when its done with the story and how to communicate to the next circle (and then there are feedback loops!).</p>
<p>Its really about effective communication and community. I didn‚Äôt get this last part until attending a session with <a href="https://nonodename.com/post/davidhussman/">David Hussman</a> who talks about building community around a story.</p>



 ]]></description>
  <category>agile</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-01-25-when-is-a-story-prepared/</guid>
  <pubDate>Mon, 25 Jan 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-01-25-when-is-a-story-prepared/blog-circles.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>What is software architecture?</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-01-25-what-is-software-architecture/</link>
  <description><![CDATA[ 





<p>Lots of folks that don‚Äôt know what I do (and some that do) will often ask what is a software architect? What is software architecture?</p>
<p>My short answer is that <strong>software architecture answers all the ‚Äúhow do I <fill-in-the-blank>‚Äù (or ‚Äúhow does it?‚Äù) questions that come up on a software development project</fill-in-the-blank></strong>. I‚Äôm a fan of the concepts behind the <a href="http://en.wikipedia.org/wiki/4%2B1_Architectural_View_Model">4+1 model of software architecture</a> - that there are various categories (views) of these questions all answered as the team works through the functional requirements (scenarios). I can never remember what the actual 4 views are - but one of the main categories is about how development is done (the development view), or where code is actually deployed and running (the physical view) to how layers of the software work together (the logical view) and then there is some other one‚Ä¶which is where I lump all the ‚Äúhow does the system do X‚Äù answers (I think its the process view - and yes, I‚Äôm too lazy to search for the answer right now).</p>
<p>When you look at all the ‚Äúhow do I‚Äù questions that come up - there are lots - but they are all architecture. This can literally be something as simple as ‚Äúhow do I add logging to this system?‚Äù with simple answers - we are using SLF4J on Log4J. Which can then lead into deeper questions (how do I change logging levels at runtime? how is logging started and shutdown in this system?)‚Ä¶up to the ‚Äústandard‚Äù stuff that architects typically focus on - ‚Äúhow does the solution provide for scalable performance?‚Äù, etc.</p>
<p>But in the end, whenever I hear a ‚Äúhow do I‚Äù or ‚Äúhow does it‚Äù question - that is architecture - and a potential teaching moment. In my opinion, architects should be doing what the other team members are doing (coding/testing) in order to be effective. And really good architects teach.</p>



 ]]></description>
  <category>architecture</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-01-25-what-is-software-architecture/</guid>
  <pubDate>Mon, 25 Jan 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-01-25-what-is-software-architecture/teaching.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>QA versus QC</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-01-25-qa-versus-qc/</link>
  <description><![CDATA[ 





<p>For Agile projects, I often coach about the need for a QA (quality assurance) role in addition to just testers (or QC / quality control).</p>
<p>For me, QA answers the question <strong>‚Äúare we doing the right job?‚Äù</strong> and QC answers the question <strong>‚Äúare we doing the job right?‚Äù</strong>.</p>
<p>I see QA working with the Customer/Product Owner on coverage for acceptance and functional testing. A great QA person will be able to answer the architecture (‚Äúhow do I - ?‚Äù) questions for the QC team as well as, like a great Business Analyst, be able to hold the domain model in their head. Could even be the same head (BA/QA)‚Ä¶</p>



 ]]></description>
  <category>agile</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-01-25-qa-versus-qc/</guid>
  <pubDate>Mon, 25 Jan 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-01-25-qa-versus-qc/checkbox.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Discovering software architecture</title>
  <dc:creator>Brian Repko</dc:creator>
  <link>https://brianrepko.github.io/blog/posts/2010-01-25-discovering-software-architecture/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://brianrepko.github.io/blog/posts/2010-01-25-discovering-software-architecture/blog-arch-func.jpg" class="quarto-figure quarto-figure-center figure-img" height="300"></p>
</figure>
</div>
<p>I draw this picture a lot on projects that I‚Äôve been on so I figured that I should put it up here on the blog. The idea behind it is that on agile projects, <strong>one discovers the architectural requirements in conjunction with discovering the functional requirements</strong>. You learn a bit about the functionality, you make some choices on architecture, you learn a bit more about functionality, you make some more choices about architecture, etc. until the full solution is complete.</p>
<p>There is a skill to choosing which architectural requirements need to be addressed when. On one project that I was on, I put off deciding on how to handle exceptions through the various layers of theh architecture. We eventually tackled the issue (with ingenious input from the team) but by then we had lots of code to change and refactor. That lesson painfully showed the cost of ‚Äútechnical debt‚Äù - we borrowed that time from the future of the project and had to pay it back with interest. Lesson learned - I would not put that concern/requirement off that late again.</p>
<p>I think that html style guides (what css classes are we using and for what purpose) on web-based projects are another common concern that gets put off and the price is paid later with interest. I‚Äôve done whole iterations of nothing but styling.</p>
<p>The trick is to make those architectural choices at the <strong>last responsible moment</strong> - but you never really know when that is. Its like knowing how to play an instrument - practice, practice, practice.</p>
<p>Are there other that you like to see addressed earlier than when you‚Äôve actually done them?</p>



 ]]></description>
  <category>agile</category>
  <category>architecture</category>
  <guid>https://brianrepko.github.io/blog/posts/2010-01-25-discovering-software-architecture/</guid>
  <pubDate>Mon, 25 Jan 2010 00:00:00 GMT</pubDate>
  <media:content url="https://brianrepko.github.io/blog/posts/2010-01-25-discovering-software-architecture/blog-arch-func.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>

[
  {
    "objectID": "posts/2025-12-31-datasetjson-part1/index.html",
    "href": "posts/2025-12-31-datasetjson-part1/index.html",
    "title": "Dataset-JSON",
    "section": "",
    "text": "As part of R/Pharma this year, I attended a workshop on the CDISC Dataset-JSON standard. This was a great workshop and props/kudos/flowers to all the folks that put that together. More recently, CDISC announced that v1.0 of the API standard is released as well as the supplementary details on compression (aka Compressed Dataset-JSON v1.1).\nThis later announcement (and some a review of Pilot 5 updates) gives me a chance to dive into this important set of specifications and updates - thus this series.\nI‚Äôm planning to do an overview in this part 1, go over some potential issues and clarifications in part 2, and then go over the REST API in part 3 (which TBH I‚Äôve not looked at in detail yet, so it goes last)."
  },
  {
    "objectID": "posts/2025-12-31-datasetjson-part1/index.html#background",
    "href": "posts/2025-12-31-datasetjson-part1/index.html#background",
    "title": "Dataset-JSON",
    "section": "Background",
    "text": "Background\nThe data sent to regulatory authorities (eg. FDA for USA, EMA for EU, PMDA for Japan, etc.) typically conforms to CDISC international standards. When transferring this (tabular) data - from sponsor to RA - the SAS V5 XPORT format (aka ‚ÄúXPT‚Äù) is currently used (required for the FDA). XPT was defined in 1989 and its use by the FDA was made official in 1999. It has many disadvantages - some of which we will highlight below.\nAlso in 1999, CDISC created the Operational Data Model (ODM). ODM is a foundational data standard that underlies many of the other CDISC standards and is XML-based. Between 2000 and 2005, ODM versions 1.0 through 1.3 were released. Some point releases, ODM v1.3.1 and v1.3.2 were released in 2010 and 2013 respectively. So from 1999 to 2013, data was defined based on ODM XML foundational standards and still transferred in XPT.\nAs industry is exploring coding alternatives to SAS - and potentially with sponsors creating all submission materials with R (or without SAS) - other data transfer format alternatives are needed. Moving to new data transfer standards will also allow CDISC to move beyond the limitations imposed by the XPT format.\nGiven that ODM was XML-based, in 2012, work was done to create Dataset-XML, with version 1.0 released in 2014. This is a XML-based data transfer format based on ODM v1.3.2. However, it was found that Dataset-XML created data files that were quite large (already an issue for XPT) and it was not widely adopted by industry.\nSkip ahead ten years, and in 2023, ODM v2.0 was released. Dataset-XML v1.0 was not updated - it is still an extension of the ODM XML schema (as far as I can tell with a cursory review) and may not need an update.\nAs part of the ODM v2.0 work, on the data transfer side, Dataset-JSON v1.0 was also released in 2023. One of the best articles on the need for, development of, and plans for Dataset-JSON is this one from Sam Hume.\nA number of hackathons and pilot projects were done - some issues found - and then Dataset-JSON v1.1 was released in Dec 2024 to address those issues. Some issues required changes to the specification - some were addressed in the User Guide. Here is the report, from PHUSE, of those pilot projects.\nLastly, as mentioned above, the Dataset-JSON API v1.0 and Compressed Dataset-JSON v1.1 standards were released a few weeks ago in Dec 2025."
  },
  {
    "objectID": "posts/2025-12-31-datasetjson-part1/index.html#dataset-json",
    "href": "posts/2025-12-31-datasetjson-part1/index.html#dataset-json",
    "title": "Dataset-JSON",
    "section": "Dataset-JSON",
    "text": "Dataset-JSON\nThe Dataset-JSON work actually has multiple components - with a view towards transfer of data via APIs instead of via files. In particular, this could become part of how sponsors can collect data from various other systems and collaborators - EDCs, CROs, etc.\nThese components are:\n\nDataset-JSON specification and schema (github repository)\n\nDataset-JSON v1.1\nThe NDJSON representation of Dataset-JSON\nCompressed Dataset-JSON v1.1 (aka DSJC)\nDataset-JSON schema\nUser‚Äôs Guide (cdisc wiki)\n\n\nand\n\nDataset-JSON API (github repository)\n\nSpecification as HTML or OpenAPI\nUser‚Äôs Guide"
  },
  {
    "objectID": "posts/2025-12-31-datasetjson-part1/index.html#issues-with-xpt",
    "href": "posts/2025-12-31-datasetjson-part1/index.html#issues-with-xpt",
    "title": "Dataset-JSON",
    "section": "Issues with XPT",
    "text": "Issues with XPT\nAnyone that has looked at clinical trial data has wondered why column names are limited to 8 characters - it‚Äôs XPT.\nThe data is always tabular (rows and columns). Columns have name, label / description, type, and length and formatting options. There is no row identifier (unlike R data.frames) - access, in SAS, is via row number (POINT=) or via indices defined on top of the data (KEY=).\nHere is a quick list of issues with XPT v5:\n\nColumn/variable types - only CHARACTER (string) and DOUBLE (numeric - integer or floating point)\nColumn names are limited to 8 alphanumeric + _ characters\nColumn labels are limited to 40 characters\nCharacter values are US ASCII only with max length of 200 characters/bytes\nCharacter values are stored with padding (so, larger than they need to be)\nNumeric values are stored as IBM hexidecimal floating point (aka HFP, aka IBM-style double) format\n\nWhich is NOT IEEE 754, see wikipedia\nThis is why XPT is technically a binary file format - numeric data encoding in the file\n\nInability to compress files - leads to data set splitting\nThere is no internally stored metadata\n\neg. file metadata, formatting on numerics, padding for characters, date/time formatting, keys\n\n\nNote that modern versions of SAS use IEEE 754 internally for floating point data - it is only the XPT format that uses the HFP format. Also note that SAS does support date, time, and datetime variables - these are internally stored as numbers - with datetime point 0 being Jan 1, 1960 00:00:00 UTC. Actually, I have no idea how timezones work in SAS (all data is UTC and timezone is a system option?).\nThere is an XPT v8 format as well which made the following changes:\n\nColumn names are extended to 32 characters (case sensitive)\nColumn labels are extended to 256 bytes\nCharacter values are extended 32767 bytes\nIt is not clear if US ASCII is still a limitation or not (note the use of bytes vs character limits) - seems like it is limited to US ASCII\n\nIn the end, there was not much industry uptake of XPT v8 and efforts were put elsewhere."
  },
  {
    "objectID": "posts/2025-12-31-datasetjson-part1/index.html#quick-overview-of-dataset-json",
    "href": "posts/2025-12-31-datasetjson-part1/index.html#quick-overview-of-dataset-json",
    "title": "Dataset-JSON",
    "section": "Quick overview of Dataset-JSON",
    "text": "Quick overview of Dataset-JSON\nDataset-JSON is similar to XPT in that it is a single-file for tabular data.\nAs JSON (link to html version of spec here), a dataset is a single object. Dataset metadata are attributes in that object, there is a columns array for column definitions, and then a rows array-of-arrays for data values.\nColumn definitions have the following attributes - unique ID (itemOID), name, label, dataType, targetDataType, length, displayFormat, and keySequence.\nNote that dataType is the ‚Äúlogical‚Äù data type - tied to ODM - with values of string, integer, decimal, float, double, boolean, datetime, date, time, and URI.\nThe column attribute targetDataType is used for some logical types (decimals as strings, date/time/datetime as integers). We‚Äôll discuss this more in part 2. It looks like boolean values do use JSON true and false (in other places I‚Äôve seen these not used in favor of ‚ÄúY‚Äù,‚ÄúN‚Äù strings). Missing values are represented with JSON null. The empty string can be a value.\nFor the NDJSON representation - it is the same as above with the following changes:\n\nEach line is a JSON object\nRow 1 is a JSON object that is all metadata and column array\nRow 2-n are each an array of data\n\nThis is basically the rows array with ‚Äúlines‚Äù being entries in the array-of-array\n\n\nNDJSON is very useful for streaming large sets of data."
  },
  {
    "objectID": "posts/2025-12-31-datasetjson-part1/index.html#next-article",
    "href": "posts/2025-12-31-datasetjson-part1/index.html#next-article",
    "title": "Dataset-JSON",
    "section": "Next article",
    "text": "Next article\nIn part 2, I‚Äôll make a list of some of the issues that come up in reading the specification and how some of the issues found in the pilot were handled.\nOverall, I‚Äôm thrilled with this effort as it opens up clinical trial data to any programming language AND data transfer via API - but some details need some discussion. My intent is to highlight and/or clarify points related to data representation (from someone with a whole 30 year career in data engineering) and not to only criticize.\nI hope those discussions will get added to the User‚Äôs Guide or maybe more pharmaverse blog posts (like here, here, and here)."
  },
  {
    "objectID": "posts/2010-02-19-making-ant-1-8-work-like-maven-not-so-much/index.html",
    "href": "posts/2010-02-19-making-ant-1-8-work-like-maven-not-so-much/index.html",
    "title": "Making Ant 1.8 work like Maven - not so much",
    "section": "",
    "text": "I‚Äôve done Ant build files in the past that ended up working like Maven2. Mostly since it was a non-Maven shop but also because it was a way to get folks into Maven-think but by using Ant.\nNow, Ant 1.8 has been released and with it some new features that could potentially make it possible to have very modular Ant builds that would be even better than Maven2. One of the main concepts within Maven2 is the various lifecycles (clean, build/default and site) and that build tasks from plugins are bound to various parts of the lifecycle. Ant 1.8 introduced the notion of extension-points and extensionOf as well as imports and local properties - these could all be used to both create plugins (macrodefs) and our own lifecycles (sets of extension-points) and then bind them all up together in a build.xml and just import what you need - potentially from an http URL.\nWell, that was the thought‚Ä¶\nTurns out that imports are processed after the build.xml is parsed. That‚Äôs all well and good, but when an extensionOf attribute is parsed, Ant looks for the target(s) named in the extensionOf value in order to add the current target as a dependency. That requires that the target has to exist in the project and if that target is part of an import (as the documentation seems to suggest), then the target doesn‚Äôt exist (yet) at the time of parsing and you get a nice error message to that effect.\nI think that this is a design flaw in how extension-point / extensionOf is supposed to work and contradicts the example cited in the documentation - which doesn‚Äôt work.\nIts too bad because with these features, I could define my own lifecycles or even change/modify the existing ones from Maven2 to do things related to database SQL modules (create the database from all the SQL scripts and some data files) or be able to mix the SQL and java files together in the same module and add phases to the lifecycle related to database setup. This has always been something that I have to hack up the pom for anyway - which is part of why I like going back to Ant - I can change it easier when I need to.\nWork-arounds? Change the ProjectHelper/TargetHelper to deal with extensionOf attributes after the import stack is popped (and all the targets are resolved) or import the extensionOfs (the bindings or which macros get called for each step) after the extension-points are imported. I‚Äôm not a fan of the latter as I really think that the bindings are the build - execute these steps for these lifecycle stages - but if my build is just a bunch of imports, that‚Äôs not the worst of it. Or screw the use of extension-points/extensionOf and just use imports with empty targets (which is kind of what extension-points are - except that I could then create a target that gets bound to multiple extension-points with extensionOf=‚Äútarget1,target2‚Äù).\nIt does sadden me that the example cited doesn‚Äôt even work however. If I get this working, I‚Äôll post the example."
  },
  {
    "objectID": "posts/2010-07-21-extending-jasypt-aes-and-blowfish-support/index.html",
    "href": "posts/2010-07-21-extending-jasypt-aes-and-blowfish-support/index.html",
    "title": "Extending Jasypt - AES and Blowfish support",
    "section": "",
    "text": "I recently had to code Java / Perl interoperable encryption - in Perl it was using the Crypt::CBC and Crypt::Blowfish modules. The perl code was meant to be as simple as possible:\n$cipher = Crypt::CBC-&gt;new( -cipher =&gt; 'Blowfish', -key =&gt; 'password' ); \n$ciphertext = $cipher-&gt;encrypt_hex(\"This data is super secret hush hush\");\nThe key is really a passphrase that is then generated into a key and IV for use with the underlying CBC/cipher. These modules are by default compatible with OpenSSL. I thought that since this is password-based encryption, that I could use Jasypt, one of my favorite libraries. Unfortunately, Jasypt only supports the PBE* cipher algorithms and none of them are the OpenSSL standards. So then I thought that I could at least get Jasypt to support Blowfish. No luck‚Ä¶the algorithm is just hard-coded to PBE-based encryption. Even the IV work would be impossible.\nSo for the project, I created my own mini-framework that includes converters (hex/base64 string to byte[] or String to byte[] based on a character-set) and ciphers defined via generics with the ability to create a string-to-string ‚Äúencryptor‚Äù by combining a String-to-byte[] (utf-8) converter with a byte[]-to-byte[] cipher with a byte[]-to-String (hex) converter. This is sort of what Jasypt does but its not very pluggable in that fashion.\nThen to write the byte[]-to-byte[] cipher, I started with a generalized algorithm that works for both the PBE* algorithms but also for AES and Blowfish with the key and IV generation handled in the process. Plug in BouncyCastle‚Äôs OpenSSLPBEParametersGenerator for key/IV generation and write my own decorator for dealing with sharing the salt as ‚ÄúSalted__XXXXXXXX‚Äù in front of the ciphertext and voila! Perl-Java encryption interoperability based on passwords and random salts!!\nThat project has ended and I‚Äôm now in-between gigs so I worked that code into Jasypt - just added a feature request (with a patch) to Jasypt. Not specifically the perl stuff but the generalized algorithm. That allows users to finally extend Jasypt - still for password-based encryption but not limited to the PBE* algorithms. Support is finally in there for AES and Blowfish with key and IV generation based on PBKDF2 or whatever else you want to add. Changes to Jasypt to support the configuration of the whole ‚Äúpipeline‚Äù is not in there - that would require some serious changes to Jasypt.\nAs for the algorithms - they look like this:\nFor encryption,\nEncryptionData data = buildEncryptionData();\ndata.setMethodInput(message); \ndataProcessor.preProcess(Cipher.ENCRYPT_MODE, data); \nSecretKey key = keyGenerator.generateSecretKey(data); \nAlgorithmParameterSpec parameterSpec = paramGenerator.generateParameterSpec(data);\nsynchronized (encryptCipher) { \n  encryptCipher.init(Cipher.ENCRYPT_MODE, key, parameterSpec); \n  data.setCipherOutput(encryptCipher.doFinal(data.getCipherInput())); \n}\ndataProcessor.postProcess(Cipher.ENCRYPT_MODE, data);\nreturn data.getMethodOutput();\nFor decryption,\nEncryptionData data = buildEncryptionData(); \ndata.setMethodInput(encryptedMessage); \ndataProcessor.preProcess(Cipher.DECRYPT_MODE, data); \nSecretKey key = keyGenerator.generateSecretKey(data); \nAlgorithmParameterSpec parameterSpec = paramGenerator.generateParameterSpec(data); \nsynchronized (decryptCipher) { \n  this.decryptCipher.init(Cipher.DECRYPT_MODE, key, parameterSpec); \n  data.setCipherOutput(decryptCipher.doFinal(data.getCipherInput())); \n} \ndataProcessor.postProcess(Cipher.DECRYPT_MODE, data); \nreturn data.getMethodOutput();\nAnd all the real work is done in the SecretKeyGenerator (which actually generates more than just the key - it just returns the key), the AlgorithmParamsGenerator and the EncryptionDataProcessor - all of which are just interfaces. All the transient data for the method is kept in the EncryptionData class or subclass. So that is the patch just submitted.\nAnd perl interoperability could be added with an OpenSSLSecretKeyGenerator and an OpenSSLEncryptionDataProcessor to handle the ‚ÄúSalted__XXXXXXX‚Äù format - the rest is all in there (once the patch is approved, committed and released). The OpenSSLSecretKeyGenerator would work like the PBKDF2SecretKeyGenerator in that it would produce the key and IV based on a password and fixed or random salt. Its just that OpenSSL does a funky key and IV generation mechanism that I‚Äôm not sure is in the default JCE providers. And the OpenSSLEncryptionDataProcessor is just an extension of the existing one with the hardcoded ‚ÄúSalted__‚Äù thrown in for good measure.\nHere‚Äôs hoping that gets added by the next project. Jasypt team - I‚Äôm more than willing to help!"
  },
  {
    "objectID": "posts/2025-12-20-tiledb-snowflake/index.html",
    "href": "posts/2025-12-20-tiledb-snowflake/index.html",
    "title": "TileDB and Snowflake integration",
    "section": "",
    "text": "I attended a webinar / demo of TileDB Carrara and it‚Äôs integration with Snowflake a few weeks ago. You can find the recording on YouTube.\nI am familiar with TileDB as a tensor (aka n-dimensional) data format - a format often used for biomedical data. In fact, they have special formats, beyond general Arrays, for single-cell, VCF, and image data. You can find details at the TileDB Academy website.\nBut Carrara was new to me - it is a combination of ‚Äúdata‚Äù catalog (vs files) for a given project but also includes a notebook / compute environment. I found this short demo on YouTube as a good overview of Carrara. What was cool to see was how these special formats can be rendered directly in the tool but also the notion of multi-file datasets rendered as a single entry in the project.\nFor the integration, you can see your Snowflake-based tabular data in your Carrara environment. And you could also see your TileDB-based multi-dimensional data in your Snowflake data - as tabular data. This basically allows you to merge multi-dimensional and tabular data with notebooks. On the Snowflake side, this could then be used for models or other computation that is added to your Snowflake environment.\nThis made me wonder if something similar wasn‚Äôt possible for DuckDB - could I see multi-dimensional TileDB data in DuckDB as tables? This doesn‚Äôt seem to be supported and, for me as an open source advocate, solidifies the need for open source alternatives to TileDB (eg. hdf5, Zarr, COG). I‚Äôll have a later post on Zarr vs hdf5 (and more) since hdf5 doesn‚Äôt work so great on cloud-based storage.\nThat said, the ability to integrate TileDB multi-dimensional data and Snowflake together is a great expansion for multi-omic data analysis."
  },
  {
    "objectID": "posts/2014-09-28-blogging-again-with-a-change-of-focus/index.html",
    "href": "posts/2014-09-28-blogging-again-with-a-change-of-focus/index.html",
    "title": "Blogging again",
    "section": "",
    "text": "A few years back, I noticed a friend‚Äôs LinkedIn update that he was working for Entagen, a company doing life sciences work in the Twin Cities. And I knew that I had to jump at the chance to get into life science programming. I was subcontracted to do some work at Novartis - the research branch in particular (NIBR) and then quickly found myself working on a project to manage and merge all public genomic data. I‚Äôm so thankful to the people that made that all happen.\nAnd I‚Äôve never looked back. It‚Äôs been 3 years, a move to a new country (Switzerland) and a new disease area (Oncology) and I love my work. I love learning the science and seeing where good software engineering can make a difference. If ‚Äúbioinformatician‚Äù means a ‚Äúsoftware engineer that uses their knowledge of biology‚Äù - then I now call myself a bioinformatician.\nIt‚Äôs been a while since I was blogging - but I‚Äôm going to start up again. However, the blog will be more focused on the state of software engineering in the life sciences (from my one perspective) and where technology is going in this space. Stay tuned‚Ä¶"
  },
  {
    "objectID": "posts/2010-01-25-what-is-software-architecture/index.html",
    "href": "posts/2010-01-25-what-is-software-architecture/index.html",
    "title": "What is software architecture?",
    "section": "",
    "text": "Lots of folks that don‚Äôt know what I do (and some that do) will often ask what is a software architect? What is software architecture?\nMy short answer is that software architecture answers all the ‚Äúhow do I ‚Äù (or ‚Äúhow does it?‚Äù) questions that come up on a software development project. I‚Äôm a fan of the concepts behind the 4+1 model of software architecture - that there are various categories (views) of these questions all answered as the team works through the functional requirements (scenarios). I can never remember what the actual 4 views are - but one of the main categories is about how development is done (the development view), or where code is actually deployed and running (the physical view) to how layers of the software work together (the logical view) and then there is some other one‚Ä¶which is where I lump all the ‚Äúhow does the system do X‚Äù answers (I think its the process view - and yes, I‚Äôm too lazy to search for the answer right now).\nWhen you look at all the ‚Äúhow do I‚Äù questions that come up - there are lots - but they are all architecture. This can literally be something as simple as ‚Äúhow do I add logging to this system?‚Äù with simple answers - we are using SLF4J on Log4J. Which can then lead into deeper questions (how do I change logging levels at runtime? how is logging started and shutdown in this system?)‚Ä¶up to the ‚Äústandard‚Äù stuff that architects typically focus on - ‚Äúhow does the solution provide for scalable performance?‚Äù, etc.\nBut in the end, whenever I hear a ‚Äúhow do I‚Äù or ‚Äúhow does it‚Äù question - that is architecture - and a potential teaching moment. In my opinion, architects should be doing what the other team members are doing (coding/testing) in order to be effective. And really good architects teach."
  },
  {
    "objectID": "posts/2010-01-25-when-is-a-story-prepared/index.html",
    "href": "posts/2010-01-25-when-is-a-story-prepared/index.html",
    "title": "When is a story prepared?",
    "section": "",
    "text": "This is another drawing that I use a lot while coaching agile projects and actually is part of multiple discussions around agile methods. Most agile methods talk about stories being created by the customer and put on a backlog. For some iteration, at iteration planning, the story gets explained to developers and testers. They work on the story until it is complete. And we have lots of conversations, as agile coaches and teams, about ‚Äúwhen is a story complete‚Äù.\nThis misses a lot of the work that needs to be done in order to make teams effective. I like to have regular backlog grooming meetings with part of the team and ask the question - is this story prepared?. What is needed in order to bring this story to iteration planning? That needed work might involve QA (quality assurance) for acceptance tests or functional tests. That might involve UX (user experience) for wireframes or drawings. That might involve some TA (technical architecture) work or IA (information architecture, or domain modeling) work depending on the story. It might require a BA to work out the business value of this story or how to break it up into what needs to done now versus later (breaking up stories into smaller, potentially optional pieces). Its only when a story is prepared that it should be brought to iteration planning.\nI also use this picture to help explain why some work is ‚Äúon the board‚Äù for the iteration (meaning we are tracking velocity and burndown charts - its the developer/tester circle) vs work that needs to get done but we aren‚Äôt measuring velocity for it. The first is working towards completing the story. The latter is working towards getting the story prepared.\nIt also helps explain the roles of the non-customer, non-developer and non-tester folks‚Ä¶though I‚Äôm pretty careful to explain that that 2nd circle is optional work (story by story) and that that work can be done by anyone with those skills. Its really about what would make the communication of this story effective and doing that in as lightweight of a fashion as you need.\nThe last part of this drawing is that the story doesn‚Äôt stop because development is done. Its really done when its deployed (some would say deployed to production) and supported. This means that the story needs to be shared with operations and support teams. I‚Äôve seen this done as part of a release process and actually made it the responsibility of the whole team to figure how to to communicate the stories that are being released. Really each circle needs to figure out when its done with the story and how to communicate to the next circle (and then there are feedback loops!).\nIts really about effective communication and community. I didn‚Äôt get this last part until attending a session with David Hussman who talks about building community around a story."
  },
  {
    "objectID": "posts/2026-01-15-datasetjson-part2/index.html",
    "href": "posts/2026-01-15-datasetjson-part2/index.html",
    "title": "Dataset-JSON",
    "section": "",
    "text": "TipSeries\n\n\n\nThis is part 2 of a series on dataset-json, you can find part 1 here for an overview of dataset-json."
  },
  {
    "objectID": "posts/2026-01-15-datasetjson-part2/index.html#intention",
    "href": "posts/2026-01-15-datasetjson-part2/index.html#intention",
    "title": "Dataset-JSON",
    "section": "Intention",
    "text": "Intention\nSimilar to how art can be additive (painting) or subtractive (sculpting), computational work can be positive (‚Äúhow do I get to a result‚Äù) or negative (‚Äúwhat could go wrong here‚Äù). Most of us develop in both ways of working - and we tend to have a default / stronger side. Mine is the negative - which is why I think I‚Äôve always focused on risk with systems development. I mention this because this blog post falls 100% in the - what could go wrong here category. This is me going through a specification and calling out anything that feels risky. I hope this spawns conversations and github issues and wiki updates and PRs. This is me also calling out where I‚Äôd love to contribute - not just complain. I‚Äôll also add that I am not a SAS programmer so I may definitely get some items wrong here.\nFrom part 1, here is the link to the html version of the spec - so let‚Äôs dive in‚Ä¶"
  },
  {
    "objectID": "posts/2026-01-15-datasetjson-part2/index.html#background---xml-and-json-technologies",
    "href": "posts/2026-01-15-datasetjson-part2/index.html#background---xml-and-json-technologies",
    "title": "Dataset-JSON",
    "section": "Background - XML and JSON technologies",
    "text": "Background - XML and JSON technologies\nWithin the XML set of specifications and technologies - we have the following:\n\nXML - data format\nXML Schema - structure and data types with simple validation and constraints\nSchematron - rule-based content validation, complex relationships, and business logic\n\nFor JSON we have a similar set of specifications and technologies:\n\nJSON - data format\nJSON schema - structure and data types with simple validation and constraints\njsontron - a port of Schematron for JSON (not widely used)\n\nThe reason that I want to bring this up is that some of the issues below relate to ‚Äúwhere‚Äù something belongs and potentially having it in the wrong space.\nAlso, I think that the use of jsontron would be interesting in this space potentially.\n\nData Types\nJSON, JSON schema, and dataset-json work with a set of data types. Note that JSON / JSON schema are working on object / property definition and dataset-json is column definition. Here is a table that goes over what is available for each specification in relation to each other:\n\nData Types\n\n\n\n\n\n\n\nJSON\nJSON schema\ndataset-json\n\n\n\n\nboolean\nboolean\nboolean\n\n\nstring\nstring\nstring\n\n\n\n\ndecimal\n\n\n\n\ndatetime\n\n\n\n\ndate\n\n\n\n\ntime\n\n\n\n\nURI\n\n\nnumber\ninteger\ninteger\n\n\n\nnumber\nfloat\n\n\n\n\ndouble\n\n\narray\narray\n-\n\n\nobject\nobject\n-\n\n\n\nNote that dataset-json has both a JSON schema and LinkML version of the schema. The JSON schema is based on the 2019-09 standard (vs the latest, which is 2020-12).\nAlso note that JSON schema has various constraints added to the type. In general, objects can also specify, via a required array, which properties are required. There is also the capability to do ‚Äúrequired-if‚Äù logic (which could also be a jsontron rule).\nFor string-based data types, we have minLength, maxLength, pattern, and format. JSON schema has known formats for date-time, date, time, and uri (and iri) - but dataset-json does not use these. It makes use of pattern for validation of those string-based types. This may be the case that pattern is actually used when validating and format may not be. That is probably dependent on the validator software. You can find the full list of supported JSON schema formats here.\nFor numeric data, JSON schema separates integers (‚Ñ§) from decimal (ùîª) data. Note that decimals (ùîª) is a subset of the rationals (‚Ñö) - given by \\(a/10^n\\) (where \\(a\\) and \\(n\\) are in ‚Ñ§). Thus \\(1/3\\) is in ‚Ñö but not in ùîª (it repeats forever) and thus for some values we have to deal with both precision and rounding.\nThe dataset-json data types of float and double are intended to represent in-memory IEEE 754 single- and double-precision constructs. It is basically saying what data type to use in the environment (SAS, R, python, etc.) that will import this data - or at least the capability of said environment (eg. R only has doubles - not floats). There are the well-known issues of converting values in ùîª to a base 2 binary format.\nInterestingly, there is no JSON schema format for dataset-json‚Äôs ‚Äúdecimal‚Äù - again that is using a data type to specify the in-memory representation for this column (aka decimal stored as a string).\nIn terms of character encoding for the file, later versions of JSON require UTF-8 encoding. The dataset-json User‚Äôs Guide refers to optional usage of UTF-16 or UTF-32 but this is not allowed in RFC8259.\nAlso, a general issue with JSON is that the Unicode escape - \\uXXXX - only supports Unicode‚Äôs Basic Multilingual Plane (BMP) and not full Unicode. One needs to either embed the codepoints directly (UTF-8 supports all Unicode) or escape as surrogate pair.\nAnd then for moving from US-ASCII to Unicode‚Ä¶\n\nOnce you have Unicode strings, there are many issues that one needs to worry about. I‚Äôm not going to go over all of these but here is a short list (many from here).\n\nUnicode normalization\nlength of a Unicode string (with grapheme clusters - particularly for Asian languages)\ncasing (upper/lower)\nsorting / collation\nright-to-left vs left-to-right languages\nregex class membership (eg. :alpha:)\n\nI get that we are looking to move beyond the current XPT limitations at a later stage. At some point, I‚Äôm sure CDISC will have a number of workshops on ‚ÄúInternationalizing Clinical Trial Data‚Äù."
  },
  {
    "objectID": "posts/2026-01-15-datasetjson-part2/index.html#our-problem-domain",
    "href": "posts/2026-01-15-datasetjson-part2/index.html#our-problem-domain",
    "title": "Dataset-JSON",
    "section": "Our Problem Domain",
    "text": "Our Problem Domain\nI think it‚Äôs useful to also step back and share what the actual problem we are trying to solve here is. We will have, as part of the data and coding related to a clinical trial, ‚Äúdata frames‚Äù. For SAS, these are datasets, for R, these are data.frame-s and for python, these are pandas.DataFrame-s (or polars?) and in SQL, these are tables.\nWe have two main contexts - same-language serialization (export / import) and different-language serialization. In both these contexts, the ultimate question is - is the original in-memory data frame the same as the resulting in-memory data frame. When we are in the same-language context (SAS ‚Üí JSON ‚Üí SAS), ‚Äúsameness‚Äù of in-memory data frames can consider deeper properties of the internal data structure. When we are in the different-language context (R ‚Üí JSON ‚Üí SAS), ‚Äúsameness‚Äù of the in-memory data frames does not have access to those deeper properties - internal data types used for the columns are in different programming languages and data frame classes have different properties - SAS datasets can have a ‚Äúkey‚Äù (hash), multiple indices, display formats etc. R data.frames have row.names which SAS does not (similar to key but SAS keys don‚Äôt have to be unique).\nThe reason that I bring this up is that some parts of the dataset-json specification are for the SAS-specific same-language context.\nI actually think that we might not care about a different-language context as clinical trial validation from the regulatory authority will be same-language. However, as clinical trial code becomes more polyglot - some SAS, some R, some python - then we might need to consider different-language contexts."
  },
  {
    "objectID": "posts/2026-01-15-datasetjson-part2/index.html#oids-and-the-model-beyond-a-data-frame",
    "href": "posts/2026-01-15-datasetjson-part2/index.html#oids-and-the-model-beyond-a-data-frame",
    "title": "Dataset-JSON",
    "section": "OIDs and the model beyond a ‚Äúdata frame‚Äù",
    "text": "OIDs and the model beyond a ‚Äúdata frame‚Äù\nAs I was initially reviewing the dataset-json specification, one quick item that stuck out was that there was no way to say if a column allowed for missing values or not - that is - is a column required.\nThis led me into the ODM model and the various OID columns in the specification.\nThe dataset-json specification is not ONLY ‚Äúserialized data frames‚Äù - it is ‚Äúserialized data frames within a catalog‚Äù. That catalog is the define.xml that is sent along with the .json or .ndjson or .dsjc files. This gets a bit too deep and needs it‚Äôs own blog post but in short - you can think of the following connections.\n\n\n\nODM term\nconcept\n\n\n\n\nStudy\nStudy\n\n\nItemGroup\nDomain or data frame metadata\n\n\nItem\nVariable or column metadata\n\n\nItemGroupData\nactual row of values\n\n\nItemData\nactual cell value\n\n\nFile\nactual filename\n\n\n\nSo an ItemGroupOID might have a value of IG.DM for the SDTM DM domain (Demographics). And there will be a ItemOID of IT.STUDYID or IT.USUBJID in that item group. Those are columns that used in multiple domains. The DM domain might also have an ItemOID of IT.DM.ETHNIC which is specific to the DM domain.\nThe reason that I bring this up is that some properties of the column definition are in define.xml as ItemRef(s) and some properties as ItemDef(s). Mandatory is part of the ItemRef. The dataset-json specification is only ItemDef data.\nThe specification is written so that the OID columns are optional, but to fully validate data, there are some column attributes that are only in define.xml."
  },
  {
    "objectID": "posts/2026-01-15-datasetjson-part2/index.html#column-attribute-issues",
    "href": "posts/2026-01-15-datasetjson-part2/index.html#column-attribute-issues",
    "title": "Dataset-JSON",
    "section": "Column Attribute Issues",
    "text": "Column Attribute Issues\nLooking at the column attributes, what is required are the basics\n\nitemOID - the unique key of this column\nname - the column name\nlabel - the column label\ndataType - the column type\n\nThese all make sense - with dataType being an enum (see list above). Technically, there are many other dataTypes in the ODM XML model but in practice the main types are there.\n\ntargetDataType\nAs for targetDataType, this came about as datetime, date, and time values in SAS are either strings or numeric values (with the 0-date being Jan 1, 1960 and the 0-time being midnight). But the data in the JSON file is always a string, so this column attribute is telling the serialization software what the resulting in-memory data type should be. But R and python have various classes for datetime, date, and time values. Some of those do use a number representation for timestamps under the covers. But again, it seems we are using the specification to specify details of the internal data structure.\nThis column attribute is also used for the decimal data type in a superfluous manner. It has to be added for string-representations of decimal values.\nIf we really want to specify the language-specific data structure properties, then it might make more sense to have those under the top-level sourceSystem perhaps - or under language-specific top-level attributes.\n\n\nlength\nNext we have length, which as mentioned above is problematic for Unicode strings based on normalization and grapheme clustering. The sense I get is that the value here is meant to be a capability (you have to handle strings at-least this long).\n\n\ndisplayFormat\nThis is clearly a SAS only attribute to used to make sure original and resulting datasets are the ‚Äúsame‚Äù.\n\n\nkeySequence\nThe keySequence attribute can be used to create the SAS dataset key (hash) - so this is clearly for internal data structure. Technically in SAS, this can doesn‚Äôt have to be unique (eg. MULTIDATA: 'Y'). For R, row.names could be created from this but row.names must be unique.\nI‚Äôm not familiar enough with SAS to know if a component of a key implies it is required.\nI‚Äôd also add that SAS datasets can have multiple indices, so I‚Äôm not sure why, if we are specifying internal data structure we include the key and not the indices. My guess is that these are not part of ODM XML schema to which we are creating this specification from. It could be the ODM XML schema is a bit SAS-specific as well.\n\n\nWhat is not here\nAs mentioned above, if I needed to specify more rules to validate a dataset, then I would need to know if the column values can be missing or not (required) or conditionally required.\nIn addition, all those JSON schema properties start to apply - range on numeric values? enumerations? code-lists? string regex?"
  },
  {
    "objectID": "posts/2026-01-15-datasetjson-part2/index.html#top-level-metadata-attribute-issues",
    "href": "posts/2026-01-15-datasetjson-part2/index.html#top-level-metadata-attribute-issues",
    "title": "Dataset-JSON",
    "section": "Top-level Metadata Attribute Issues",
    "text": "Top-level Metadata Attribute Issues\nAgain, the required attributes for the dataset-json top-level metadata all make sense.\n\nname - the name of the dataset\nlabel - the label for the dataset\ncolumns - the column definitions (see above)\nrecords - the number of rows\ndatasetJSONCreationDateTime - timestamp for dataset-json file creation\ndatasetJSONVersion - the version (1.1 with optional third component)\nitemGroupOID - the OID for this dataset\n\nIn general, it is a pet peeve of mine to allow datetime and time values without timezones but that is allowed here (as well as dealing with incomplete data) - all part of ISO 8601.\nAnd for style, I‚Äôd have used rowCount for records but given the context it makes total sense.\n\nOIDs\nThe fileOID, studyOID, metaDataVersionOID and metaDataRef attributes are all related to connecting this data to the define.xml catalog - technically optional but I‚Äôm not sure how to properly validate a dataset without it.\nIn the end, itemGroupOID and itemOID are the only required OID values.\n\n\ndbLastModifiedDateTime\nThe dbLastModifiedDateTime attribute is the last modified timestamp for the source of this JSON data.\nThere is a jsontron type rule for this being earlier than the creation timestamp required attribute.\nAgain as a style thing, I would have used ‚Äúsource‚Äù vs ‚Äúdb‚Äù - and perhaps made this part of the sourceSystem object. And it should have a timezone.\nThe mention of ‚Äúdb‚Äù however, makes me wonder about creating a DuckDB extension to read and write dataset-json files (each file as a table and then include the catalog data as well).\n\n\nSource information\nThe originator, sourceSystem (with name and version) are to be used to share the organization and ‚Äúsystem‚Äù that generated this dataset-json file. It‚Äôs not clear to me if sourceSystem should be a package name, programming language, internal SCE name? I think more clarity on how this could be used is warranted. This could also be a place there SAS, R, python-specific data could be recorded."
  },
  {
    "objectID": "posts/2026-01-15-datasetjson-part2/index.html#extendability-issues",
    "href": "posts/2026-01-15-datasetjson-part2/index.html#extendability-issues",
    "title": "Dataset-JSON",
    "section": "Extendability Issues",
    "text": "Extendability Issues\nThere is discussion of extendability to dataset-json - this uses a new sourceSystem.systemExtensions attribute but also seems to imply a custom JSON schema file. I do think JSON schema has built in schema extension capabilities and perhaps those could be looked at in the examples. Perhaps this is a place to extend the data to internal data structure details."
  },
  {
    "objectID": "posts/2026-01-15-datasetjson-part2/index.html#next-article",
    "href": "posts/2026-01-15-datasetjson-part2/index.html#next-article",
    "title": "Dataset-JSON",
    "section": "Next article",
    "text": "Next article\nI hope this is useful and I am more than open to creating github issues / updates to the User Guide etc. Happy to receive feedback - my contact info is at my website.\nIn part 3, I‚Äôll review the API and potential issues there."
  },
  {
    "objectID": "posts/2010-02-26-extreme-distributed-scrum-daily-standup/index.html",
    "href": "posts/2010-02-26-extreme-distributed-scrum-daily-standup/index.html",
    "title": "Extreme distributed scrum - daily standup",
    "section": "",
    "text": "I‚Äôve worked on Scrum teams where 1/2 the team is in one location and 1/2 in another (both offshore and onshore) and every now and then we would use an IM conference in order to have a ‚Äústandup‚Äù (except that we are sitting and on IM). We tried video and phone conferencing as well but given the lag in the network as well as lack of equipment and network availability, IM seemed to just work better. IM allowed for give-and-take (with some lag but a lag we were familiar with) and was always available. In addition, IM allowed the conversation to be sent via email for later reading and sharing (to the 1/2 of the team that wasn‚Äôt in yet). Since then, I‚Äôve wondered about what technology tools one would need if the team was completely separated (think rock stars all working from home).\nIf all the team members were located in their own location - how would I set this up? The kicker here is the mess of timezones that might be in the mix. Obviously, I‚Äôd have a wiki and perhaps an agile PM (kanban/scrum) web app running somewhere that we could all access in our timezone. When folks are distributed and have an overlapping time, then can use VNC (or other free solutions like that) to ‚Äúpair-up‚Äù as needed. Likewise, VoIP/IM conferences or just VoIP/IM for issues and/or questions.\nBut how to do ‚Äústandups‚Äù when there isn‚Äôt a time that everyone can standup? How to let someone know you are stuck on something and how to hand off a potential solution to someone that will get it hours later. My insight was that a team could host an internal blog/Twitter to share what they did yesterday, what they are doing today and what issues are blocking them. Status updates (‚Äúworking on X‚Äù or ‚Äúcan‚Äôt figure out Y‚Äù) can then really be done at anytime and those folks that are online can help step in. Some IM systems have a status but I‚Äôm not sure that that is very visible. Add an RSS feed on top of the teams blogs (like twitter) and you‚Äôll start to see team collaboration. Start your day by reading all the updates from folks since you were last on. The whole project life could be read if you really wanted too - like emailing the IM discussion. Still doesn‚Äôt help with the I have an issue with X and what a potential solution might be (hours later). I could see the wiki or issue tracking system kind of work in that space. In this scenario, the world of ‚Äústandup‚Äù starts to look like ‚Äústatus‚Äù - but that might be ok just given the realities of multiple people in multiple timezones. I thought that it would be an interesting way to run a project without standups.\nIteration and release planning would be difficult in this situation - that might just have to be done together.\nI‚Äôve not had this situation - have you? What worked or didn‚Äôt work?"
  },
  {
    "objectID": "posts/2025-10-31-blogging-for-real/index.html",
    "href": "posts/2025-10-31-blogging-for-real/index.html",
    "title": "Blogging again‚Ä¶for real",
    "section": "",
    "text": "The 2014 post that I wrote was about my move into bioinformatics as well as a move to Switzerland. Specifically, working in the Oncology disease area of Novartis Biomedical Research.\nThe 2019 post that I wrote had me back in Minneapolis and working for Carrot Health. Still related to health - data engineering for predictive models in health care (social determinants of health, healthcare quality, etc.).\nSo, you can tell I did NOT start blogging again - and now it‚Äôs been 6 years.\nBased on COVID work protocols, Novartis allowed for remote work in the disease areas and a position opened up with the Oncology team. I was fortunate to return in Oct 2020. I was even more fortunate to be able to retire, just before my 60th birthday, in June 2025.\nThus I can say that I truly will be blogging more - for real. I just converted my old Wordpress blog into this Quarto-based one.\nAnd I have a bunch of projects - some are based in Minneapolis. I‚Äôm serving as the Twin Cities city captain for Bits-in-Bio. I‚Äôm also spending more time with ISAIAH - a pro-democracy multi-racial, multi-faith group here in Minnesota.\nOn the bioinformatics side, there are also a ton of projects.\n\nA system for bringing Specification by Example to R libraries - starting with the pharmaverse\nA system for collecting genome and gene references in parquet / duckdb\nSingle-cell / Spatial dataset conversion with duckdb (using the hdf5 extension)\nImprovements to the OMOP CDM data model\nVCFs at scale (potentially with Zarr)\nPotentially some algorithms using simplicial topology with multi-omic datasets (spatial transcriptomics + H&E stain images)\nand lots more (literally, I have a page of potential projects)\n\nHere in Minnesota, residents at age 62 can attend classes at the UMN for free. In a few years, I‚Äôll be going back to school but I‚Äôm not sure what classes I‚Äôll be pursuing.\nIn short, stay tuned - there will be more getting published here. I‚Äôll also add notifications to new blog posts on Mastodon and LinkedIn - links you can find at the top of this page."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning, Thinking, and Coding",
    "section": "",
    "text": "Dataset-JSON\n\n\n\npharmaverse\n\ndata\n\n\n\nPart 2 - potential issues and challenges\n\n\n\n\n\nJan 15, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nDataset-JSON\n\n\n\npharmaverse\n\ndata\n\n\n\nPart 1 - intro to the series\n\n\n\n\n\nDec 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTileDB and Snowflake integration\n\n\n\nbioinformatics\n\ndata\n\n\n\nWebinar on data integration\n\n\n\n\n\nDec 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nValidation of R libraries for FDA/EMA submissions\n\n\n\npharmaverse\n\nagile\n\n\n\nR/Pharma talk about quarto, gherkin, and cucumber\n\n\n\n\n\nNov 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging again‚Ä¶for real\n\n\n\nbioinformatics\n\n\n\nand lots more time to do it\n\n\n\n\n\nOct 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSnowflake for Biomedical Research\n\n\n\narchitecture\n\nbioinformatics\n\n\n\n10 reasons why‚Ä¶\n\n\n\n\n\nApr 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging again\n\n\n\nbioinformatics\n\n\n\n‚Ä¶with a change of focus\n\n\n\n\n\nSep 28, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nSlides from Practical Agility\n\n\n\nagile\n\njava\n\n\n\nJBehave and FIT - Good/Bad/Ugly\n\n\n\n\n\nSep 22, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nJBehave 3.0 released!!\n\n\n\nagile\n\njava\n\n\n\n‚Ä¶and some contributions\n\n\n\n\n\nSep 2, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nExtending Jasypt - AES and Blowfish support\n\n\n\njava\n\n\n\nEncryption between Java and Perl\n\n\n\n\n\nJul 21, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nJBehave presentation for Twin Cities JUG\n\n\n\nagile\n\njava\n\n\n\nBack to presenting after a hiatus\n\n\n\n\n\nApr 12, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nExtreme distributed scrum - daily standup\n\n\n\nagile\n\n\n\nThoughts on asynchronous agile work\n\n\n\n\n\nFeb 26, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Ant 1.8 work like Maven - not so much\n\n\n\njava\n\n\n\nPublishing my failed experiments\n\n\n\n\n\nFeb 19, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nConnecting Agile Teams - one pink post-it at a time\n\n\n\nagile\n\n\n\nCommunicating via color - old school, I know\n\n\n\n\n\nJan 27, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nScrum and Kanban together\n\n\n\nagile\n\n\n\nPerhaps it‚Äôs and and not or\n\n\n\n\n\nJan 27, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nWhen is a story prepared?\n\n\n\nagile\n\n\n\nIt‚Äôs really about community and effective communication\n\n\n\n\n\nJan 25, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is software architecture?\n\n\n\narchitecture\n\n\n\nIt‚Äôs more about the questions\n\n\n\n\n\nJan 25, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nQA versus QC\n\n\n\nagile\n\n\n\nThese are different in important ways\n\n\n\n\n\nJan 25, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nDiscovering software architecture\n\n\n\nagile\n\narchitecture\n\n\n\nNotes on emergent functionality and architecture when developing a software system\n\n\n\n\n\nJan 25, 2010\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-04-09-snowflake-for-biomedical-research/index.html",
    "href": "posts/2019-04-09-snowflake-for-biomedical-research/index.html",
    "title": "Snowflake for Biomedical Research",
    "section": "",
    "text": "I‚Äôve since left biomedical research (the Genomics team at Novartis Institutes for Biomedical Research - NIBR) and am now doing health analytics with Carrot Health. At Carrot Health, we are making use of Snowflake Computing as our data storage and query system. I love Snowflake and there are so many features that make it better than what we had at NIBR. In the spirit of ‚ÄúTop 10 Cool Things I Like About Snowflake‚Äù, I bring you my 10 reasons that Snowflake works for biomedical research.\nReason 1 - All in the Cloud (AWS or Azure) - no DBA, no hardware, no tuning.\nYou can be up and running instantly in your cloud of choice. There are even safety features like UNDROP, should you DROP a table or view by mistake. For bio folks - if you are a small lab and don‚Äôt have DBAs and hardware admins, etc., then no problem. And if you do, then the security features (below) should be enough to get your IT department to buy-in.\nReason 2 - Persistent (cached) results (with a visual execution plan)\nIn Snowflake, if you (or anyone) executes a query with a result set, that result is cached and used either in other query plans or as a direct result with the same SQL. This might mean that the first time you execute a query it might need some horsepower, but then for 24 hours after that, you‚Äôll get that data back instantly. This is great for bio folks that are typically working on a particular project‚Äôs data - joined with large public datasets. Those can be cached for a while and then when you are on to the next project, they will just get removed from cache. And if you aren‚Äôt sure, you can easily go into Query History and look at the visual execution plan.\nReason 3 - The functions you need - pivot / unpivot, analytic functions and UDFs\nLet‚Äôs face it - bio folks like their data in matrix form sometimes - pivot and unpivot in the database is great. Having the ability to do a wide variety of analytic functions can help with basic statistics. And having the ability to add your own functions is great too - but ECMAscript only.\nReason 4 - JSON in SQL\nSnowflake supports a VARIANT column type that can hold JSON data and it has the SQL extensions to query that data. This is super useful for mixing structured and semi-structured data together. And that is key for aggregating bio data - because we can almost agree on most of the structure but then everyone has their extra data that they want to keep.\nReason 5 - Can connect from anything\nSnowflake supports ODBC, JDBC, Python, Spark, a web console, and it‚Äôs own snowsql command. You can basically use any tool to get connected. We were able to easily add support for SchemaSpy and Flyway (JDBC-based tools) for Snowflake - and I typically use DbVisualizer (JDBC) to access it.\nReason 6 - Like a data lake but with SQL\nSnowflake has amazing capabilities to both load and unload data to and from S3 (we are in AWS and now Azure) and it‚Äôs fast. You can regularly point it at a folder and it knows which files have already been loaded. And you can define that process to happen automagically. There are some file formats that it doesn‚Äôt support out of the box - I‚Äôve had to convert some fixed width data to separated-values - but that is minor compared to the built-in infrastructure. For bio folks, I think that this is awesome - getting scientists to put their data in S3 is far easier than helping them get it into the database.\nReason 7 - Data Sharing, Data Cloning, and Time Travel\nSnowflake has the ability to share databases between accounts. This means that someday, we could have reference data already loaded (once) in Snowflake and have everyone share it. Or results of a consortium‚Äôs work could be shared once. Sharing WITH SQL access. There is also the ability to quickly clone data which is another way that one can share data / parts of data (or promote data from QA to PROD). Snowflake, like Datomic, also has the ability to return results based on the data at a given time. For bio folks, this is exactly what is needed for reproducible research - and/or - for data that changes over time but you don‚Äôt want to deal with formal versioning.\nReason 8 - Multiple Databases and Schemas\nSnowflake is one of the few systems that supports multiple databases with multiple schemas in them. And all SQL can cross databases and schemas. This helps tremendously with data organization and potentially with role-based sharing rights. And security doesn‚Äôt stop there - data is encrypted at rest and in transit and you can even lock down access to your own AWS PrivateLink so traffic never leaves your combined data center / AWS cloud. Snowflake is HIPAA and SOC2 compliant as well.\nReason 9 - Scaling compute vs scaling storage\nWith Snowflake, the SQL execution ‚Äúcluster‚Äù is called a ‚Äúwarehouse‚Äù (horrible name, I know, but there you are). One can size (and resize) a warehouse for the queries at hand - thus having the ability to scale at will as needed (there are even warehouse clusters to get you even more compute if you need). You pay separately for storage and compute but you have tremendous control over it (and access to the accounting). You can even has department only warehouses to enable chargeback policies.\nReason 10 - the bleeding edge is available\nSnowflake supports parquet files as well. It would be awesome to try and use ADAM-formatted data - or heck, run a whole Big Data Genomics variant calling pipeline directly on the database. Or could be fun to try a version of hail.is directly on the database. This is something I‚Äôd love to see people try - and is only do-able in Snowflake.\nSo there are my 10 - please feel free comment or email me at brian -dot- repko -at- learnthinkcode -dot- com. I should add that I‚Äôm writing this as a way to share my experience with my past co-workers and Snowflake has not asked for this or is supporting this in anyway. My thoughts and opinions only."
  },
  {
    "objectID": "posts/2010-01-27-connecting-agile-teams-one-pink-post-it-at-a-time/index.html",
    "href": "posts/2010-01-27-connecting-agile-teams-one-pink-post-it-at-a-time/index.html",
    "title": "Connecting Agile Teams - one pink post-it at a time",
    "section": "",
    "text": "I‚Äôve used color coded cards on Scrum boards - green for user story, blue for system story, yellow task cards for design or review work, blue task card for ‚Äútechnical architecture work‚Äù, etc. - lots of variations. Sometimes I suggest it, sometimes I don‚Äôt. Just part of the box of tools. The one thing that I‚Äôve noticed with this however was the use of pink cards and post-its and how they can be used to connect agile teams and help build an agile enterprise.\nMy original use of pink cards was for a Scrum board for developers to fix a critical or blocker bug. This was for a team that was just developers - the testers were a completely different team. And the pink card was basically a request for the development team to help un-block the testing team (and dev team would estimate it and decide if they would need to take something else off the board).\nI‚Äôve also used pink post-its on Scrum boards to report a blocking issue on a task - just as a way to remind people working on the issue that it is important and to bring it up in standup until the issue is resolved.\nOn another team that I worked on we sort-of had a Kanban board for release or operations tasks related to the program (multiple projects - one operations team) and if the implementation team had a request to make of them (e.g.¬†database to setup), then we would create a pink card for their board. Basically a pink card is a Please Do This ASAP request. Maybe pink should stand for Please Implement Now, Kind (Sir/Madam).\nWhat I realized is that one way to connect these teams, with their own boards and tasks and stories is that the issue (post-it) is tied to request(s) to resolve the issue (cards) and you could track and connect those issues/tasks that way. So basically, for that first scenario (separate dev and test teams), if the test team had had a board, their pink post-it (the blocking issue) was tied to the pink card for the development team (the issue resolvers). And a board with a lot of pink is a conversation waiting to happen.\nSimple and easy way to handle and track issues that need to get done now that I think helps build an agile enterprise."
  },
  {
    "objectID": "posts/2010-09-22-slides-from-practical-agility-jbehave-and-fit-goodbadugly/index.html",
    "href": "posts/2010-09-22-slides-from-practical-agility-jbehave-and-fit-goodbadugly/index.html",
    "title": "Slides from Practical Agility",
    "section": "",
    "text": "I presented a ‚ÄúLightning Talk‚Äù (6 minutes) at the last Practical Agility meeting on ‚ÄúJBehave and FIT - the Good, the Bad and the Ugly‚Äù. For the talk everything was on NEON cards (neon-green, neon-yellow and neon-red! - its all I could find at Walgreens) and before throwing them out thought that I would put them into powerpoint (minus the neon) and share. Slides are up on SlideShare - but you might want to download as all the notes are in the notes section. Enjoy!"
  },
  {
    "objectID": "posts/2025-11-06-quarto-gherkin/index.html",
    "href": "posts/2025-11-06-quarto-gherkin/index.html",
    "title": "Validation of R libraries for FDA/EMA submissions",
    "section": "",
    "text": "Since R/Pharma 2023 in Chicago, I‚Äôve wanted to look into using the techniques of ATDD (or BDD‚Ä¶ or Specification by Example) for testing sets of R packages (aka - an R library). This was a topic that came up as part of the R Validation Hub work that was presented at that conference. Particularly around the discussion on how can multiple pharma companies share their R library validation tests. It was a chance to bring a technique from software engineering that I was familiar with (in Java and Python) to the world of R and in particular the pharmaverse.\nIt‚Äôs now 2 years later and I will have a pre-recorded talk at the (free!) virtual R/Pharma 2025 conference today (Nov 6, 2025). R/Pharma makes all the sessions available in about a month on YouTube - this is a community that shares and grows together after all - but here are the early links to where I host things.\n\nThe pre-recorded video is hosted here\nThe slides are hosted here\n\nWhile putting this together, I found this link from Gojko Adzic, author of ‚ÄúSpecification by Example‚Äù on where this technique sits in our bag of software engineering tools after 10 years - now 15 years - later. It validates that software quality is enhanced by the use of this technique - even if used for requirements only - versus being used for both requirements and automated testing. It also validates that the use of the Gherkin-based frameworks - Given/When/Then - are the norm - and fortunately, for R, we now have Jakub Sobolewski‚Äôs {cucumber} package.\nThe more I think about what this could look like - and potentially integrated into Quarto - I see the following:\n\nThe ‚Äúliving documentation‚Äù is a Quarto website with potentially additional descriptive explanation around gherkin text (along with sessionInfo() output)\nThe website can be organized into pages - Quarto documents with ```gherkin code-blocks that contain the actual gherkin text\nQuarto could be configured with a gherkin engine - for R, that could be knitr+cucumber (and can imagine a python engine as well)\nQuarto rendering invokes knitr and cucumber to execute the gherkin tests and formats the output for pandoc to render the results as part of the report\nSteps could be registered on the Quarto page in an ```{r} block or in a package and discovered on package load\nThe report could be included as part of the validation report\n\nI‚Äôm super interested in people‚Äôs thoughts on this - please reach out via links at the end of the slides."
  },
  {
    "objectID": "posts/2010-04-12-jbehave-presentation-for-twin-cities-java-users-group/index.html",
    "href": "posts/2010-04-12-jbehave-presentation-for-twin-cities-java-users-group/index.html",
    "title": "JBehave presentation for Twin Cities JUG",
    "section": "",
    "text": "Finally finished the JBehave presentation for tonight‚Äôs Twin Cities Java Users Group. You can find the PPT and source code at the LearnThinkCode website. Any and all feedback is welcome.\nIn the end, I really think that this is the year that Agile testing via executable requirements will take off and I do think that JBehave can be a part of that. There are some key things to work out however (library bundling issues, integration with JUnit for Spring) that need to be looked at before its really ready for prime time. And getting the Pico Ajax Email / Selenium example working was painful. Please don‚Äôt release versioned software that depends on snapshot releases of other code!\nSo, it would be usable on projects if you are willing to spend some time on getting your base class / infrastructure stuff setup. Personally, I like it more than FIT/Fitnesse."
  },
  {
    "objectID": "posts/2010-01-25-qa-versus-qc/index.html",
    "href": "posts/2010-01-25-qa-versus-qc/index.html",
    "title": "QA versus QC",
    "section": "",
    "text": "For Agile projects, I often coach about the need for a QA (quality assurance) role in addition to just testers (or QC / quality control).\nFor me, QA answers the question ‚Äúare we doing the right job?‚Äù and QC answers the question ‚Äúare we doing the job right?‚Äù.\nI see QA working with the Customer/Product Owner on coverage for acceptance and functional testing. A great QA person will be able to answer the architecture (‚Äúhow do I - ?‚Äù) questions for the QC team as well as, like a great Business Analyst, be able to hold the domain model in their head. Could even be the same head (BA/QA)‚Ä¶"
  },
  {
    "objectID": "posts/2010-01-25-discovering-software-architecture/index.html",
    "href": "posts/2010-01-25-discovering-software-architecture/index.html",
    "title": "Discovering software architecture",
    "section": "",
    "text": "I draw this picture a lot on projects that I‚Äôve been on so I figured that I should put it up here on the blog. The idea behind it is that on agile projects, one discovers the architectural requirements in conjunction with discovering the functional requirements. You learn a bit about the functionality, you make some choices on architecture, you learn a bit more about functionality, you make some more choices about architecture, etc. until the full solution is complete.\nThere is a skill to choosing which architectural requirements need to be addressed when. On one project that I was on, I put off deciding on how to handle exceptions through the various layers of theh architecture. We eventually tackled the issue (with ingenious input from the team) but by then we had lots of code to change and refactor. That lesson painfully showed the cost of ‚Äútechnical debt‚Äù - we borrowed that time from the future of the project and had to pay it back with interest. Lesson learned - I would not put that concern/requirement off that late again.\nI think that html style guides (what css classes are we using and for what purpose) on web-based projects are another common concern that gets put off and the price is paid later with interest. I‚Äôve done whole iterations of nothing but styling.\nThe trick is to make those architectural choices at the last responsible moment - but you never really know when that is. Its like knowing how to play an instrument - practice, practice, practice.\nAre there other that you like to see addressed earlier than when you‚Äôve actually done them?"
  },
  {
    "objectID": "posts/2010-01-27-scrum-and-kanban-together/index.html",
    "href": "posts/2010-01-27-scrum-and-kanban-together/index.html",
    "title": "Scrum and Kanban together",
    "section": "",
    "text": "One of my favorite links is Henrik Kniberg‚Äôs ‚Äúmini-book‚Äù on Kanban and Scrum and how they work (and how they are similar and different). Given that description of Kanban and my thoughts on story prep and story release work, I would really love to try a Kanban board for story prep and release work with a Scrum board for implementation work. Definitely for prep and implementation - release probably depends how that process looks - perhaps one release board for a program (multiple projects but one solution).\n\n\n\n\n\nBacklog grooming would actively work the story prep board. The team could see what stories are getting ready for planning as well as a release team (or management team) seeing what is getting ready for release to production. I think that it would actually engage those team members that are at the daily standup but are not developers or testers - they can point to what they are working on - its just on the story prep kanban board. I think that it could make for a good information radiator for a wider ‚Äúteam‚Äù.\nThe other way to look at this, from a metrics standpoint, is to see that the whole Scrum board is just one column of a larger Kanban board and that you could measure and reduce the throughput time of a story from backlog to released to production on that larger Kanban board.\nHas anyone ever done anything like this? Did it work? Things to improve about it?\nPutting this post together, I just noticed Henrik‚Äôs Kanban and Scrum - Making the Most of Both - something new to read!"
  },
  {
    "objectID": "posts/2010-09-02-jbehave-3-0-released/index.html",
    "href": "posts/2010-09-02-jbehave-3-0-released/index.html",
    "title": "JBehave 3.0 released!!",
    "section": "",
    "text": "JBehave 3.0 was released yesterday (finally!!). I‚Äôm thrilled to have donated the Multi-tennant Spring Security example (which has been updated to JBehave 3). That example is now part of the many examples that are included in JBehave. Looking to update the presentation on my website to explain some of the new features that make up JBehave 3.0. Congrats to Mauro and Paul on all their hard work!"
  }
]